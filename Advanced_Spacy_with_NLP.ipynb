{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Advanced Spacy with NLP.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bharathkumar-kancharla/Natural-Language-Processing/blob/master/Advanced_Spacy_with_NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "APvsWVeUBQTD",
        "colab_type": "text"
      },
      "source": [
        "This NoteBook based on the course [Advanced spaCy with NLP](https://course.spacy.io/)\n",
        "\n",
        "\n",
        "Required files available [here](https://github.com/ines/spacy-course/tree/master/exercises)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3RoKzzUjPwJY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import the English language class\n",
        "from spacy.lang.en import English\n",
        "\n",
        "# Create the nlp object (Contains the processing pipeline)\n",
        "# It contains all the different components in the pipeline.\n",
        "# It also includes language-specific rules used for tokenizing the text into words and punctuation.\n",
        "\n",
        "nlp = English()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DWLeHCq_Q2lw",
        "colab_type": "text"
      },
      "source": [
        "**The Doc object**\n",
        "\n",
        "When we process a text with the nlp object, spaCy creates a Doc object – short for \"document\". The Doc lets us access information about the text in a structured way, and no information is lost.\n",
        "\n",
        "The Doc behaves like a normal Python sequence by the way and lets you iterate over its tokens, or get a token by its index."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FhhwFQMWQLzg",
        "colab_type": "code",
        "outputId": "c025ecdb-bc8f-4fc3-c4b4-98f9c1cd8ba6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# Created by processing a string of text with the nlp object\n",
        "doc = nlp(\"Hello world!\")\n",
        "\n",
        "# Iterate over tokens in a Doc\n",
        "for token in doc:\n",
        "    print(token.text)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Hello\n",
            "world\n",
            "!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cD1wqZ9YRb-j",
        "colab_type": "text"
      },
      "source": [
        "**The Token object**\n",
        "\n",
        "Token objects represent the tokens in a document – for example, a word or a punctuation character.\n",
        "\n",
        "To get a token at a specific position, you can index into the Doc.\n",
        "\n",
        "Token objects also provide various attributes that let you access more information about the tokens. For example, the dot text attribute returns the verbatim token text.\n",
        "\n",
        "![alt text](https://course.spacy.io/doc.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N1He2M2BREfk",
        "colab_type": "code",
        "outputId": "a384b80a-0927-4bba-90ca-48671099242c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "doc = nlp(\"Hello world!\")\n",
        "\n",
        "# Index into the Doc to get a single Token\n",
        "token = doc[1]\n",
        "\n",
        "# Get the token text via the .text attribute\n",
        "print(token.text)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "world\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQhJ56SJSBNu",
        "colab_type": "text"
      },
      "source": [
        "**The Span object**\n",
        "\n",
        "A Span object is a slice of the document consisting of one or more tokens. It's only a view of the Doc and doesn't contain any data itself.\n",
        "\n",
        "To create a Span, you can use Python's slice notation. \n",
        "\n",
        "![Span Object](https://course.spacy.io/doc_span.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GFc_LQ_NSAdg",
        "colab_type": "code",
        "outputId": "0b8eedb3-db63-4db9-eb19-3a10c75bc499",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# A slice from the Doc is a Span object\n",
        "span = doc[1:4]\n",
        "\n",
        "# Get the span text via the .text attribute\n",
        "print(span.text)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "world!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BqmaKarKScV7",
        "colab_type": "text"
      },
      "source": [
        "**Lexical Attributes**\n",
        "\n",
        "we can see some of the available token attributes:\n",
        "\n",
        "\"i\" is the index of the token within the parent document.\n",
        "\n",
        "\"text\" returns the token text.\n",
        "\n",
        "\"is alpha\", \"is punct\" and \"like num\" return boolean values indicating whether the token consists of alphanumeric characters, whether it's punctuation or whether it resembles a number. For example, a token \"10\" – one, zero – or the word \"ten\" – T, E, N.\n",
        "\n",
        "These attributes are also called lexical attributes: **they refer to the entry in the vocabulary and don't depend on the token's context.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8sWP-NMgS5wX",
        "colab_type": "code",
        "outputId": "01ad17e0-0e5d-448a-a52e-82371aea4e24",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "doc = nlp(\"It costs $5.\")\n",
        "\n",
        "print('Index:   ', [token.i for token in doc])\n",
        "print('Text:    ', [token.text for token in doc])\n",
        "\n",
        "print('is_alpha:', [token.is_alpha for token in doc])\n",
        "print('is_punct:', [token.is_punct for token in doc])\n",
        "print('like_num:', [token.like_num for token in doc])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Index:    [0, 1, 2, 3, 4]\n",
            "Text:     ['It', 'costs', '$', '5', '.']\n",
            "is_alpha: [True, True, False, False, False]\n",
            "is_punct: [False, False, False, False, True]\n",
            "like_num: [False, False, False, True, False]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GfxrjzIbUFrU",
        "colab_type": "text"
      },
      "source": [
        "**imports for other languages:**\n",
        "\n",
        "German : from spacy.lang.de import German\n",
        "\n",
        "Spanish : from spacy.lang.es import Spanish"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v7RR5mqcUYjB",
        "colab_type": "code",
        "outputId": "5af689bc-53a5-4843-e188-68a5eebaa9a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "from spacy.lang.en import English\n",
        "\n",
        "nlp = English()\n",
        "\n",
        "# Process the text\n",
        "doc = nlp(\n",
        "    \"In 1990, more than 60% of people in East Asia were in extreme poverty. \"\n",
        "    \"Now less than 4% are.\"\n",
        ")\n",
        "\n",
        "# Iterate over the tokens in the doc\n",
        "for token in doc:\n",
        "    # Check if the token resembles a number\n",
        "    if token.like_num:\n",
        "        # Get the next token in the document\n",
        "        next_token = doc[token.i + 1]\n",
        "        # Check if the next token's text equals '%'\n",
        "        if next_token.text == \"%\":\n",
        "            print(\"Percentage found:\", token.text)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Percentage found: 60\n",
            "Percentage found: 4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "risqoBQwX2yR",
        "colab_type": "text"
      },
      "source": [
        "# Statistical models\n",
        "\n",
        "\n",
        "**What are statistical models?**\n",
        "1. Enable spaCy to predict linguistic attributes in context\n",
        "  \n",
        "  *   Part-of-speech tags\n",
        "  *   Syntactic dependencies\n",
        "  *   Named entities\n",
        "\n",
        "2. Models are trained on labeled example texts\n",
        "\n",
        "3. Can be updated with more examples to fine-tune predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o0PKw8sSZBP-",
        "colab_type": "text"
      },
      "source": [
        "**Model Packages**\n",
        "\n",
        "- spaCy provides a number of pre-trained model packages you can download using the \"spacy download\" command. \n",
        "\n",
        "  For example, the \"*en_core_web_sm*\" package is a small English model that supports all core capabilities and is <font color = 'blue'>trained on web text.</font>\n",
        "\n",
        "- The spacy dot load method loads a model package by name and returns an nlp object.\n",
        "\n",
        "- The package provides the binary weights that enable spaCy to make predictions.\n",
        "\n",
        "- It also includes the vocabulary, and meta information to tell spaCy which language class to use and how to configure the processing pipeline."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bNZnyDrZY7_Z",
        "colab_type": "code",
        "outputId": "33a00338-2a71-41d7-f3ea-3071f2b1b356",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        }
      },
      "source": [
        "# Download the English model package\n",
        "!python -m spacy download en_core_web_sm\n",
        "\n",
        "import spacy\n",
        "# Load the package\n",
        "nlp = spacy.load('en_core_web_sm')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: en_core_web_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz#egg=en_core_web_sm==2.2.5 in /usr/local/lib/python3.6/dist-packages (2.2.5)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.21.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.2)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.18.2)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.6.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.3)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (46.0.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.38.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2019.11.28)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.8)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.5.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.1.0)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qUehAFySaXYv",
        "colab_type": "text"
      },
      "source": [
        "**Predicting Part-of-speech Tags**\n",
        "\n",
        "- Let's take a look at the model's predictions. In this example, we're using spaCy to predict part-of-speech tags, the word types in context.\n",
        "\n",
        "- First, we load the small English model and receive an nlp object.\n",
        "\n",
        "- Next, we're processing the text \"She ate the pizza\".\n",
        "\n",
        "- For each token in the Doc, we can print the text and the \"pos underscore\" attribute, the predicted part-of-speech tag.\n",
        "\n",
        "- In spaCy, attributes that return strings usually end with an underscore – attributes <font color='blue'>without the underscore return an ID.</font>\n",
        "\n",
        "- Here, the model correctly predicted \"ate\" as a verb and \"pizza\" as a noun."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w0_SPuqTZybV",
        "colab_type": "code",
        "outputId": "3d88efe6-ed8f-4c6b-b2d6-77d74dea027e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "# Load the small English model\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Process a text\n",
        "doc = nlp(\"She ate the pizza\")\n",
        "\n",
        "# Iterate over the tokens\n",
        "for token in doc:\n",
        "    # Print the text and the predicted part-of-speech tag\n",
        "    print(token.text, token.pos_)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "She PRON\n",
            "ate VERB\n",
            "the DET\n",
            "pizza NOUN\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aIHuvQZUa360",
        "colab_type": "code",
        "outputId": "62236e94-a5d2-40a9-9c5e-7d3c36deab45",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "token = doc[1]\n",
        "token.text,token.pos"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('ate', 100)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JfxyY1mTbdWR",
        "colab_type": "text"
      },
      "source": [
        "**Predicting Syntactic Dependencies**\n",
        "\n",
        "- In addition to the part-of-speech tags, we can also predict how the words are related. For example, whether a word is the subject of the sentence or an object.\n",
        "\n",
        "- The \"dep underscore\" attribute returns the predicted dependency label.\n",
        "\n",
        "- The head attribute returns the syntactic head token. You can also think of it as the parent token this word is attached to."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Iq55dB4bloL",
        "colab_type": "code",
        "outputId": "469e997e-64a9-429a-f920-4dbd1e1af407",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "for token in doc:\n",
        "    print(token.text, token.pos_, token.dep_, token.head.text)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "She PRON nsubj ate\n",
            "ate VERB ROOT ate\n",
            "the DET det pizza\n",
            "pizza NOUN dobj ate\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QzsC9bIMbyW7",
        "colab_type": "text"
      },
      "source": [
        "**Dependency label scheme**\n",
        "\n",
        "![Dependency label](https://course.spacy.io/dep_example.png)\n",
        "\n",
        "\n",
        "- To describe syntactic dependencies, spaCy uses a standardized label scheme. Here's an example of some common labels:\n",
        "\n",
        "- The pronoun \"She\" is a nominal subject attached to the verb – in this case, to \"ate\".\n",
        "\n",
        "- The noun \"pizza\" is a direct object attached to the verb \"ate\". It is eaten by the subject, \"she\".\n",
        "\n",
        "- The determiner \"the\", also known as an article, is attached to the noun \"pizza\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWQ_TePS2K22",
        "colab_type": "text"
      },
      "source": [
        "**Predicting Named Entities**\n",
        "\n",
        "![Named Entities](https://course.spacy.io/ner_example.png)\n",
        "\n",
        "- Named entities are \"real world objects\" that are assigned a name – for example, a person, an organization or a country.\n",
        "\n",
        "- The doc dot ents property lets you access the named entities predicted by the model.\n",
        "\n",
        "- It returns an iterator of Span objects, so we can print the entity text and the entity label using the \"label underscore\" attribute.\n",
        "\n",
        "- In this case, the model is correctly predicting \"Apple\" as an organization, \"U.K.\" as a geopolitical entity and \"$1 billion\" as money."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ERX5BUD1bt72",
        "colab_type": "code",
        "outputId": "0aae2a25-e1b9-4de6-b632-79fabda5a6cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# Process a text\n",
        "doc = nlp(u\"Apple is looking at buying U.K. startup for $1 billion\")\n",
        "\n",
        "# Iterate over the predicted entities\n",
        "for ent in doc.ents:\n",
        "    # Print the entity text and its label\n",
        "    print(ent.text, ent.label_)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Apple ORG\n",
            "U.K. GPE\n",
            "$1 billion MONEY\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "irZraxw64cX5",
        "colab_type": "text"
      },
      "source": [
        "**TIP: the explain method**\n",
        "\n",
        "- Get quick definitions of the most common tags and labels\n",
        "\n",
        "- \"GPE\" for geopolitical entity isn't exactly intuitive – but spacy dot explain can tell you that it refers to - countries, cities and states.\n",
        "\n",
        "- The same works for part-of-speech tags and dependency labels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MhixV-j343bg",
        "colab_type": "code",
        "outputId": "97f0d984-dc26-46a9-9f67-17539658c715",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "spacy.explain('GPE')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Countries, cities, states'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rAGwpYNx48kP",
        "colab_type": "code",
        "outputId": "cf8cdd39-b3d7-4dbc-b395-1c1eadda3141",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "spacy.explain('NNP')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'noun, proper singular'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zLuM6mnB4-ji",
        "colab_type": "code",
        "outputId": "319dd152-6bba-4ac1-e05b-b68b7774eec8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "spacy.explain('dobj')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'direct object'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C7C5UEqX5NuA",
        "colab_type": "text"
      },
      "source": [
        "**Note:** Statistical models allow you to generalize based on a set of training examples. Once they’re trained, they use binary weights to make predictions. That’s why it’s not necessary to ship them with their training data.\n",
        "\n",
        "\n",
        "**Q) What’s not included in a model package that you can load into spaCy?**\n",
        "\n",
        "- A meta file including the language, pipeline and license.\n",
        "\n",
        "- Binary weights to make statistical predictions.\n",
        "\n",
        "- <font color='red'>The labelled data that the model was trained on.</font>\n",
        "\n",
        "- Strings of the model's vocabulary and their hashes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tZZMiGos5mUT",
        "colab_type": "code",
        "outputId": "31a10c41-66df-4f44-a0e9-7b3a7a9b2e6e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "spacy.explain('PROPN')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'proper noun'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_s2iSm5mEylB",
        "colab_type": "text"
      },
      "source": [
        "# Ruled Based Matching\n",
        "\n",
        "**Why not just regular expressions?**\n",
        "- Match on Doc objects, not just strings\n",
        "- Match on tokens and token attributes\n",
        "- Use the model's predictions\n",
        "  Example: \"duck\" (verb) vs. \"duck\" (noun)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24UZuwvJIlAF",
        "colab_type": "text"
      },
      "source": [
        "- Compared to regular expressions, the matcher works with Doc and Token objects instead of only strings.\n",
        "\n",
        "- It's also more flexible: you can search for texts but also other lexical attributes.\n",
        "\n",
        "- You can even write rules that use the model's predictions.\n",
        "\n",
        "- For example, find the word \"duck\" only if it's a verb, not a noun."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W1Ld3ZykJB38",
        "colab_type": "text"
      },
      "source": [
        "Match patterns are lists of dictionaries. Each dictionary describes one token. The keys are the names of token attributes, mapped to their expected values.\n",
        "\n",
        "In this example, we're looking for two tokens with the text \"iPhone\" and \"X\".\n",
        "\n",
        "We can also match on other token attributes. Here, we're looking for two tokens whose lowercase forms equal \"iphone\" and \"x\".\n",
        "\n",
        "We can even write patterns using attributes predicted by the model. Here, we're matching a token with the lemma \"buy\", plus a noun. The lemma is the base form, so this pattern would match phrases like \"buying milk\" or \"bought flowers\".\n",
        "\n",
        "**Match patterns**\n",
        "- Lists of dictionaries, one per token\n",
        "\n",
        "- Match exact token texts\n",
        "\n",
        "  [{'TEXT': 'iPhone'}, {'TEXT': 'X'}]\n",
        "  \n",
        "- Match lexical attributes\n",
        "\n",
        " [{'LOWER': 'iphone'}, {'LOWER': 'x'}]\n",
        " \n",
        "- Match any token attributes\n",
        "\n",
        "  [{'LEMMA': 'buy'}, {'POS': 'NOUN'}]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z6PLRYj9KHBr",
        "colab_type": "text"
      },
      "source": [
        "1. To use a pattern, we first import the matcher from spacy dot matcher.\n",
        "\n",
        "2. We also load a model and create the nlp object.\n",
        "\n",
        "3. The matcher is initialized with the shared vocabulary, nlp dot vocab. You'll learn more about this later – for now, just remember to always pass it in.\n",
        "\n",
        "4. The matcher dot add method lets you add a pattern. The first argument is a unique ID to identify which pattern was matched. The second argument is an optional callback. We don't need one here, so we set it to None. The third argument is the pattern.\n",
        "\n",
        "5. To match the pattern on a text, we can call the matcher on any doc.\n",
        "\n",
        "This will return the matches."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y8LJQlf77c8u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import spacy\n",
        "\n",
        "# Import the Matcher\n",
        "from spacy.matcher import Matcher\n",
        "\n",
        "# Load a model and create the nlp object\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Initialize the matcher with the shared vocab\n",
        "matcher = Matcher(nlp.vocab)\n",
        "\n",
        "# Add the pattern to the matcher\n",
        "pattern = [{'TEXT': 'iPhone'}, {'TEXT': 'X'}]\n",
        "matcher.add('IPHONE_PATTERN', None, pattern)\n",
        "\n",
        "# Process some text\n",
        "doc = nlp(\"New iPhone X release date leaked\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6C63Z1aBKuWt",
        "colab_type": "code",
        "outputId": "af184390-fe4e-4d70-c7a3-c1a47515f6bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Call the matcher on the doc\n",
        "matches = matcher(doc)\n",
        "\n",
        "# Call the matcher on the doc\n",
        "doc = nlp(\"New iPhone X release date leaked\")\n",
        "matches = matcher(doc)\n",
        "\n",
        "# Iterate over the matches\n",
        "for match_id, start, end in matches:\n",
        "    # Get the matched span\n",
        "    matched_span = doc[start:end]\n",
        "    print(matched_span.text)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iPhone X\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V3sQTg8JKXZJ",
        "colab_type": "code",
        "outputId": "d2592e0e-88fd-4ee1-92db-bb47b404d3ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "matches"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(9528407286733565721, 1, 3)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FciguPYiK68V",
        "colab_type": "text"
      },
      "source": [
        "  **match_id:** hash value of the pattern name\n",
        "  **start:** start index of matched span\n",
        "  **end:** end index of matched span"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jxsD4LOnLKbQ",
        "colab_type": "code",
        "outputId": "48c53ace-a24d-4fb0-b39c-2b2b730ab7ca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "# Call the matcher on the doc\n",
        "doc = nlp(\"New iPhone X release date leaked\")\n",
        "matches = matcher(doc)\n",
        "\n",
        "# Iterate over the matches\n",
        "for match_id, start, end in matches:\n",
        "    print(match_id)\n",
        "    print(start)\n",
        "    print(end)\n",
        "    # Get the matched span\n",
        "    matched_span = doc[start:end]\n",
        "    print(matched_span.text)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "9528407286733565721\n",
            "1\n",
            "3\n",
            "iPhone X\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Z2eC6X8Lrpp",
        "colab_type": "text"
      },
      "source": [
        "**Matching lexical attributes**\n",
        "\n",
        "Here's an example of a more complex pattern using lexical attributes.\n",
        "\n",
        "We're looking for five tokens:\n",
        "\n",
        "1. A token consisting of only digits.\n",
        "\n",
        "2. Three case-insensitive tokens for \"fifa\", \"world\" and \"cup\".\n",
        "\n",
        "3. And a token that consists of punctuation.\n",
        "\n",
        "4. The pattern matches the tokens \"2018 FIFA World Cup:\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OEpvfMotLplg",
        "colab_type": "code",
        "outputId": "80944f02-cc69-4ab0-ea9d-f21f59c21bba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "pattern = [\n",
        "    {'IS_DIGIT': True},\n",
        "    {'LOWER': 'fifa'},\n",
        "    {'LOWER': 'world'},\n",
        "    {'LOWER': 'cup'},\n",
        "    {'IS_PUNCT': True}\n",
        "]\n",
        "\n",
        "doc = nlp(\"2018 FIFA World Cup: France won!\")\n",
        "\n",
        "doc"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2018 FIFA World Cup: France won!"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HkkSGugoMFg_",
        "colab_type": "text"
      },
      "source": [
        "**Matching other token attributes**\n",
        "\n",
        "In this example, we're looking for two tokens:\n",
        "\n",
        "A verb with the lemma \"love\", followed by a noun.\n",
        "\n",
        "This pattern will match \"loved dogs\" and \"love cats\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DrcTD7GAMMt9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pattern = [\n",
        "    {'LEMMA': 'love', 'POS': 'VERB'},\n",
        "    {'POS': 'NOUN'}\n",
        "]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cTnyhitUMR9w",
        "colab_type": "code",
        "outputId": "9d761de1-703f-46be-f87f-938d5921eb8b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "doc = nlp(\"I loved dogs but now I love cats more.\")\n",
        "doc"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "I loved dogs but now I love cats more."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3NceoCxbMhDI",
        "colab_type": "text"
      },
      "source": [
        "**Using operators and quantifiers**\n",
        "\n",
        "  Operators and quantifiers let you define how often a token should be matched. They can be added using the \"OP\" key.\n",
        "\n",
        "  Here, the \"?\" operator makes the determiner token optional, so it will match a token with the lemma \"buy\", an optional article and a noun."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "diwAbZuzMqs3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pattern = [\n",
        "    {'LEMMA': 'buy'},\n",
        "    {'POS': 'DET', 'OP': '?'},  # optional: match 0 or 1 times\n",
        "    {'POS': 'NOUN'}\n",
        "]\n",
        "\n",
        "doc = nlp(\"I bought a smartphone. Now I'm buying apps.\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZapSH5z1M3eg",
        "colab_type": "text"
      },
      "source": [
        "\"OP\" can have one of four values:\n",
        "\n",
        "An \"!\" negates the token, so it's matched 0 times.\n",
        "\n",
        "A \"?\" makes the token optional, and matches it 0 or 1 times.\n",
        "\n",
        "A \"+\" matches a token 1 or more times.\n",
        "\n",
        "And finally, an \"*\" matches 0 or more times.\n",
        "\n",
        "Operators can make your patterns a lot more powerful, but they also add more complexity – so use them wisely.\n",
        "\n",
        "\n",
        "Example\t      Description\n",
        "{'OP': '!'}\t       Negation: match 0 times\n",
        "{'OP': '?'}\t      Optional: match 0 or 1 times\n",
        "{'OP': '+'}     \tMatch 1 or more times\n",
        "{'OP': '*'}\t       Match 0 or more times"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K97369MGM5fp",
        "colab_type": "code",
        "outputId": "2b216b0b-6015-41f9-e5c3-9f720cdfd20e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import spacy\n",
        "\n",
        "# Import the Matcher\n",
        "from spacy.matcher import Matcher\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(\"New iPhone X release date leaked as Apple reveals pre-orders by mistake\")\n",
        "\n",
        "# Initialize the Matcher with the shared vocabulary\n",
        "matcher = Matcher(nlp.vocab)\n",
        "\n",
        "# Create a pattern matching two tokens: \"iPhone\" and \"X\"\n",
        "pattern = [{'TEXT': 'iPhone'}, {'TEXT': 'X'}]\n",
        "\n",
        "# Add the pattern to the matcher\n",
        "matcher.add(\"IPHONE_X_PATTERN\", None, pattern)\n",
        "\n",
        "# Use the matcher on the doc\n",
        "matches = matcher(doc)\n",
        "print(\"Matches:\", [doc[start:end].text for match_id, start, end in matches])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Matches: ['iPhone X']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k8F8Y3yJOPjd",
        "colab_type": "code",
        "outputId": "f7997762-1275-4c67-de74-9a67e33a04a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "doc = nlp(\n",
        "    \"i downloaded Fortnite on my laptop and can't open the game at all. Help? \"\n",
        "    \"so when I was downloading Minecraft, I got the Windows version where it \"\n",
        "    \"is the '.zip' folder and I used the default program to unpack it... do \"\n",
        "    \"I also need to download Winzip?\"\n",
        ")\n",
        "\n",
        "# Write a pattern that matches a form of \"download\" plus proper noun\n",
        "pattern = [{\"LEMMA\": \"download\"}, {\"POS\": \"PROPN\"}]\n",
        "\n",
        "# Add the pattern to the matcher and apply the matcher to the doc\n",
        "matcher.add(\"DOWNLOAD_THINGS_PATTERN\", None, pattern)\n",
        "matches = matcher(doc)\n",
        "print(\"Total matches found:\", len(matches))\n",
        "\n",
        "# Iterate over the matches and print the span text\n",
        "for match_id, start, end in matches:\n",
        "    print(\"Match found:\", doc[start:end].text)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total matches found: 3\n",
            "Match found: downloaded Fortnite\n",
            "Match found: downloading Minecraft\n",
            "Match found: download Winzip\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NsEQxKPNPjQv",
        "colab_type": "code",
        "outputId": "0537521a-35e4-4261-e9d5-04b4fe492bfe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "doc = nlp(\n",
        "    \"Features of the app include a beautiful design, smart search, automatic \"\n",
        "    \"labels and optional voice responses.\"\n",
        ")\n",
        "\n",
        "# Write a pattern for adjective plus one or two nouns\n",
        "pattern = [{\"POS\": \"ADJ\"}, {\"POS\": \"NOUN\"}, {\"POS\": \"NOUN\", \"OP\": \"?\"}]\n",
        "\n",
        "# Add the pattern to the matcher and apply the matcher to the doc\n",
        "matcher.add(\"ADJ_NOUN_PATTERN\", None, pattern)\n",
        "matches = matcher(doc)\n",
        "print(\"Total matches found:\", len(matches))\n",
        "\n",
        "# Iterate over the matches and print the span text\n",
        "for match_id, start, end in matches:\n",
        "    print(\"Match found:\", doc[start:end].text)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total matches found: 5\n",
            "Match found: beautiful design\n",
            "Match found: smart search\n",
            "Match found: automatic labels\n",
            "Match found: optional voice\n",
            "Match found: optional voice responses\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8gAk_mKzTISI",
        "colab_type": "text"
      },
      "source": [
        "# Data Structures : Vocab, Lexemes and StringStore\n",
        "\n",
        "**Shared vocab and string store**\n",
        "\n",
        "- **Vocab:** stores data shared across multiple documents\n",
        "- To save memory, spaCy encodes all strings to hash values\n",
        "- Strings are only stored once in the StringStore via nlp.vocab.strings\n",
        "- **String store:** lookup table in both directions\n",
        "\n",
        "spaCy stores all shared data in a vocabulary, the Vocab.\n",
        "\n",
        "This includes words, but also the labels schemes for tags and entities.\n",
        "\n",
        "To save memory, all strings are encoded to hash IDs. If a word occurs more than once, we don't need to save it every time.\n",
        "\n",
        "Instead, spaCy uses a hash function to generate an ID and stores the string only once in the string store. The string store is available as nlp dot vocab dot strings.\n",
        "\n",
        "It's a lookup table that works in both directions. You can look up a string and get its hash, and look up a hash to get its string value. Internally, spaCy only communicates in hash IDs.\n",
        "\n",
        "Hash IDs can't be reversed, though. If a word in not in the vocabulary, there's no way to get its string. That's why we always need to pass around the shared vocab."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1gw6tLXGTHl6",
        "colab_type": "code",
        "outputId": "d19730de-9c06-48f5-ac51-3b0e28a4063d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        }
      },
      "source": [
        "coffee_hash = nlp.vocab.strings['coffee']\n",
        "coffee_string = nlp.vocab.strings[coffee_hash]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-6bf993b79d83>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcoffee_hash\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'coffee'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcoffee_string\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcoffee_hash\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32mstrings.pyx\u001b[0m in \u001b[0;36mspacy.strings.StringStore.__getitem__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: \"[E018] Can't retrieve string for hash '3197928453018144401'. This usually refers to an issue with the `Vocab` or `StringStore`.\""
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "69Rq9bBpUbp1",
        "colab_type": "code",
        "outputId": "e6753c15-63c5-44ec-92ab-cf224c598f1a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        }
      },
      "source": [
        "# Raises an error if we haven't seen the string before\n",
        "string = nlp.vocab.strings[3197928453018144401]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-e42f0c7cc266>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mstring\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3197928453018144401\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32mstrings.pyx\u001b[0m in \u001b[0;36mspacy.strings.StringStore.__getitem__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: \"[E018] Can't retrieve string for hash '3197928453018144401'. This usually refers to an issue with the `Vocab` or `StringStore`.\""
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Swht0e_iUtrT",
        "colab_type": "text"
      },
      "source": [
        "To get the hash for a string, we can look it up in nlp dot vocab dot strings.\n",
        "\n",
        "To get the string representation of a hash, we can look up the hash.\n",
        "\n",
        "A Doc object also exposes its vocab and strings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SuX51EBqUpPn",
        "colab_type": "code",
        "outputId": "c55118eb-3658-48f4-d911-26615fa1babc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "doc = nlp(\"I love coffee\")\n",
        "print('hash value:', nlp.vocab.strings['coffee'])\n",
        "print('string value:', nlp.vocab.strings[3197928453018144401])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "hash value: 3197928453018144401\n",
            "string value: coffee\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fOxzTBCH4DBB",
        "colab_type": "code",
        "outputId": "e5651c58-05c7-4e0f-9883-361159287ba8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#The doc also exposes the vocab and strings\n",
        "doc = nlp(\"I love coffee\")\n",
        "print('hash value:', doc.vocab.strings['coffee'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "hash value: 3197928453018144401\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EXMTnwYm4g4K",
        "colab_type": "text"
      },
      "source": [
        "**Lexemes: entries in the vocabulary**\n",
        "\n",
        "- Lexemes are *context-independent* entries in the vocabulary.\n",
        "\n",
        "- You can get a lexeme by *looking up a string or a hash ID* in the vocab.\n",
        "\n",
        "- Lexemes expose attributes, just like tokens.\n",
        "\n",
        "- They hold context-independent information about a word, like the text, or whether the the word consists of alphanumeric characters.\n",
        "\n",
        "- Lexemes don't have part-of-speech tags, dependencies or entity labels. Those depend on the context."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7AwnXtRL4ZRZ",
        "colab_type": "code",
        "outputId": "973fe78e-c4c8-40c3-ffec-56fcf97f80bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "doc = nlp(\"I love coffee\")\n",
        "lexeme = nlp.vocab['coffee']\n",
        "\n",
        "# Print the lexical attributes\n",
        "print(lexeme.text, lexeme.orth, lexeme.is_alpha)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "coffee 3197928453018144401 True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jeDpZvLX5Cev",
        "colab_type": "text"
      },
      "source": [
        "Contains the **context-independent** information about a word\n",
        "- **Word text:** `lexeme.text` and `lexeme.orth` (the hash)\n",
        "- Lexical attributes like `lexeme.is_alpha`\n",
        "- **Not** context-dependent part-of-speech tags, dependencies or entity labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FH0gegxj5eo8",
        "colab_type": "text"
      },
      "source": [
        "**Vocab, hashes and lexemes**\n",
        "\n",
        "![alt text](https://course.spacy.io/vocab_stringstore.png)\n",
        "\n",
        "The Doc contains words in context – in this case, the tokens \"I\", \"love\" and \"coffee\" with their part-of-speech tags and dependencies.\n",
        "\n",
        "Each token refers to a lexeme, which knows the word's hash ID. To get the string representation of the word, spaCy looks up the hash in the string store."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rkP9ThUT54sJ",
        "colab_type": "code",
        "outputId": "b86fb227-23e6-4941-f26f-ec20e3254f57",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(\"I have a cat\")\n",
        "\n",
        "# Look up the hash for the word \"cat\"\n",
        "cat_hash = nlp.vocab.strings[\"cat\"]\n",
        "print(cat_hash)\n",
        "\n",
        "# Look up the cat_hash to get the string\n",
        "cat_string = nlp.vocab.strings[cat_hash]\n",
        "print(cat_string)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5439657043933447811\n",
            "cat\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m9DpPIe2645e",
        "colab_type": "code",
        "outputId": "1f5b2635-9510-4ffd-e894-91d757b6e7ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "doc = nlp(\"David Bowie is a PERSON\")\n",
        "\n",
        "# Look up the hash for the string label \"PERSON\"\n",
        "person_hash = nlp.vocab.strings[\"PERSON\"]\n",
        "print(person_hash)\n",
        "\n",
        "# Look up the person_hash to get the string\n",
        "person_string = nlp.vocab.strings[person_hash]\n",
        "print(person_string)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "380\n",
            "PERSON\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lC-HyOOy7fPS",
        "colab_type": "code",
        "outputId": "8b1b7590-7fa7-4aee-eedb-444504fcba38",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 249
        }
      },
      "source": [
        "# Why does this code throw an error?\n",
        "\n",
        "from spacy.lang.en import English\n",
        "from spacy.lang.de import German\n",
        "\n",
        "# Create an English and German nlp object\n",
        "nlp = English()\n",
        "nlp_de = German()\n",
        "\n",
        "# Get the ID for the string 'Bowie'\n",
        "bowie_id = nlp.vocab.strings['Bowie']\n",
        "print(bowie_id)\n",
        "\n",
        "# Look up the ID for 'Bowie' in the vocab\n",
        "print(nlp_de.vocab.strings[bowie_id])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2644858412616767388\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-cec14808b51b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# Look up the ID for 'Bowie' in the vocab\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnlp_de\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbowie_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32mstrings.pyx\u001b[0m in \u001b[0;36mspacy.strings.StringStore.__getitem__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: \"[E018] Can't retrieve string for hash '2644858412616767388'. This usually refers to an issue with the `Vocab` or `StringStore`.\""
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "369TLKEA78tB",
        "colab_type": "text"
      },
      "source": [
        "**Answer:** The string `Bowie` isn't in the German vocab, so the hash can't be resolved in the string store.\n",
        "\n",
        "**Reason:** Hashes can’t be reversed. To prevent this problem, add the word to the new vocab by processing a text or looking up the string, or use the same vocab to resolve the hash back to a string."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tK1nY_j98f8e",
        "colab_type": "text"
      },
      "source": [
        "**The Doc object**\n",
        "\n",
        "- The Doc is one of the central data structures in spaCy. It's created automatically when you process a text with the nlp object. But you can also instantiate the class manually.\n",
        "\n",
        "- After creating the nlp object, we can import the Doc class from spacy dot tokens.\n",
        "\n",
        "- Here we're creating a Doc from three words. The spaces are a list of boolean values indicating whether the word is followed by a space. Every token includes that information – even the last one!\n",
        "\n",
        "- The Doc class takes three arguments: the shared vocab, the words and the spaces."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u-YF0IKK8NHH",
        "colab_type": "code",
        "outputId": "2e8eb1d0-a9fe-4318-96b2-a8fa0ba626c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Create an nlp object\n",
        "from spacy.lang.en import English\n",
        "nlp = English()\n",
        "\n",
        "# Import the Doc class\n",
        "from spacy.tokens import Doc\n",
        "\n",
        "# The words and spaces to create the doc from\n",
        "words = ['Hello', 'world', '!']\n",
        "spaces = [True, False, False]\n",
        "\n",
        "# Create a doc manually\n",
        "doc = Doc(nlp.vocab, words=words, spaces=spaces)\n",
        "doc"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Hello world!"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ioCL6PA488UT",
        "colab_type": "text"
      },
      "source": [
        "**The Span object**\n",
        "\n",
        "![alt text](https://course.spacy.io/span_indices.png)\n",
        "\n",
        "A Span is a slice of a Doc consisting of one or more tokens. The Span takes at least three arguments: the doc it refers to, and the start and end index of the span. Remember that the end index is exclusive!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-e8xZHfG9VGE",
        "colab_type": "text"
      },
      "source": [
        "- To create a Span manually, we can also import the class from spacy dot tokens. We can then instantiate it with the doc and the span's start and end index.\n",
        "\n",
        "- To add an entity label to the span, we first need to look up the string in the string store. We can then provide it to the span as the label argument.\n",
        "\n",
        "- The doc dot ents are writable, so we can add entities manually by overwriting it with a list of spans."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "02B1cym89I0f",
        "colab_type": "code",
        "outputId": "d4721d2f-535d-4b9b-e2fc-1a5eaa788133",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# Import the Doc and Span classes\n",
        "from spacy.tokens import Doc, Span\n",
        "\n",
        "# The words and spaces to create the doc from\n",
        "words = ['Hello', 'world', '!']\n",
        "spaces = [True, False, False]\n",
        "\n",
        "# Create a doc manually\n",
        "doc = Doc(nlp.vocab, words=words, spaces=spaces)\n",
        "\n",
        "# Create a span manually\n",
        "span = Span(doc, 0, 2)\n",
        "print(span)\n",
        "\n",
        "# Create a span with a label\n",
        "span_with_label = Span(doc, 0, 2, label=\"GREETING\")\n",
        "print(span_with_label)\n",
        "\n",
        "# Add span to the doc.ents\n",
        "doc.ents = [span_with_label]\n",
        "print(doc.ents)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Hello world\n",
            "Hello world\n",
            "(Hello world,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HjYCzsSG9z-D",
        "colab_type": "text"
      },
      "source": [
        "**Tips and Tricks**\n",
        "- The Doc and Span are very powerful and optimized for performance. They give you access to all references and relationships of the words and sentences.\n",
        "\n",
        "- If your application needs to output strings, make sure to convert the doc as late as possible. If you do it too early, you'll lose all relationships between the tokens.\n",
        "\n",
        "- To keep things consistent, try to use built-in token attributes wherever possible. For example, **token.i** for the token index.\n",
        "\n",
        "Also, don't forget to always pass in the shared vocab!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OrM8fyw1-Kuh",
        "colab_type": "code",
        "outputId": "22d0953b-f61a-424b-b061-c8afe8de5d01",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Import the Doc class\n",
        "from spacy.tokens import Doc\n",
        "\n",
        "# Desired text: \"Go, get started!\"\n",
        "words = [\"Go\", \",\", \"get\", \"started\", \"!\"]\n",
        "spaces = [False, True, True, False, False]\n",
        "\n",
        "# Create a Doc from the words and spaces\n",
        "doc = Doc(nlp.vocab, words=words, spaces=spaces)\n",
        "print(doc.text)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go, get started!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rNDlXv804lSj",
        "colab_type": "code",
        "outputId": "bce39ea0-f3ec-4cb4-ffa4-d66dab90f9d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "from spacy.lang.en import English\n",
        "\n",
        "nlp = English()\n",
        "\n",
        "# Import the Doc and Span classes\n",
        "from spacy.tokens import Doc, Span\n",
        "\n",
        "words = [\"I\", \"like\", \"David\", \"Bowie\"]\n",
        "spaces = [True, True, True, False]\n",
        "\n",
        "# Create a doc from the words and spaces\n",
        "doc = Doc(nlp.vocab, words=words, spaces=spaces)\n",
        "print(doc.text)\n",
        "\n",
        "# Create a span for \"David Bowie\" from the doc and assign it the label \"PERSON\"\n",
        "span = Span(doc, 2, 4, label=\"PERSON\")\n",
        "print(span.text, span.label_)\n",
        "\n",
        "# Add the span to the doc's entities\n",
        "doc.ents = [span]\n",
        "\n",
        "# Print entities' text and labels\n",
        "print([(ent.text, ent.label_) for ent in doc.ents])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "I like David Bowie\n",
            "David Bowie PERSON\n",
            "[('David Bowie', 'PERSON')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jLsyiqxM5XeO",
        "colab_type": "text"
      },
      "source": [
        "The code in this example is trying to analyze a text and collect all proper nouns that are followed by a verb."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BiNDmnsT5ayu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(\"Berlin is a nice city\")\n",
        "\n",
        "# Get all tokens and part-of-speech tags\n",
        "token_texts = [token.text for token in doc]\n",
        "pos_tags = [token.pos_ for token in doc]\n",
        "\n",
        "for index, pos in enumerate(pos_tags):\n",
        "    # Check if the current token is a proper noun\n",
        "    if pos == \"PROPN\":\n",
        "        # Check if the next token is a verb\n",
        "        if pos_tags[index + 1] == \"VERB\":\n",
        "            result = token_texts[index]\n",
        "            print(\"Found proper noun before a verb:\", result)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ErjuAPta58S8",
        "colab_type": "text"
      },
      "source": [
        "**Why is the code bad?**\n",
        "\n",
        "**Answer:**  It only uses lists of strings instead of native token attributes. This is often less efficient, and can't express complex relationships.\n",
        "\n",
        "**Reason:** Always convert the results to strings as late as possible, and try to use native token attributes to keep things consistent.\n",
        "\n",
        "- The **.pos_** attribute returns the coarse-grained part-of-speech tag and PROPN is the correct tag to check for proper nouns.\n",
        "\n",
        "- It shouldn’t be necessary to convert strings back to **Token** objects. Instead, try to avoid converting tokens to strings if you still need to access their attributes and relationships."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x56iqfeu7Bjz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(\"Berlin is a nice city\")\n",
        "\n",
        "# Iterate over the tokens\n",
        "for token in doc:\n",
        "    # Check if the current token is a proper noun\n",
        "    if token.pos_ == \"PROPN\":\n",
        "        # Check if the next token is a verb\n",
        "        if doc[token.i + 1].pos_ == \"VERB\":\n",
        "            print(\"Found proper noun before a verb:\", token.text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zGGsooXC7Jsu",
        "colab_type": "text"
      },
      "source": [
        "**Word vectors and semantic similarity**\n",
        "\n",
        "how to use spaCy to predict how similar documents, spans or tokens are to each other.\n",
        "\n",
        "**Comparing semantic similarity**\n",
        "- `spaCy` can compare two objects and predict similarity\n",
        "- Doc.similarity(), Span.similarity() and Token.similarity()\n",
        "- Take another object and return a similarity score (0 to 1)\n",
        "  \n",
        "\n",
        "- **Important:** needs a model that has word vectors included, for example:\n",
        "          ✅ en_core_web_md (medium model)\n",
        "          ✅ en_core_web_lg (large model)\n",
        "          🚫 NOT en_core_web_sm (small model)\n",
        "          \n",
        "spaCy can compare two objects and predict how similar they are – for example, documents, spans or single tokens.\n",
        "\n",
        "The Doc, Token and Span objects have a dot similarity method that takes another object and returns a floating point number between 0 and 1, indicating how similar they are.\n",
        "\n",
        "One thing that's very important: In order to use similarity, you need a larger spaCy model that has word vectors included.\n",
        "\n",
        "For example, the medium or large English model – but not the small one. So if you want to use vectors, always go with a model that ends in \"md\" or \"lg\". You can find more details on this in the models documentation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SS9DVlPg8jHe",
        "colab_type": "code",
        "outputId": "a544e242-feec-414c-ae02-4f8aa38eafe2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Similarity examples\n",
        "# Load a larger model with vectors\n",
        "\n",
        "# Download the English model package\n",
        "# !python -m spacy download en_core_web_lg\n",
        "# issue: https://stackoverflow.com/questions/56927602/unable-to-load-the-spacy-model-en-core-web-lg-on-google-colab\n",
        "import spacy\n",
        "nlp = spacy.load('en_core_web_lg')\n",
        "\n",
        "# Compare two documents\n",
        "doc1 = nlp(\"I like fast food\")\n",
        "doc2 = nlp(\"I like pizza\")\n",
        "print(doc1.similarity(doc2))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.8627204117787385\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oQNod9_3-cYx",
        "colab_type": "code",
        "outputId": "5bedefd8-d2a3-40ee-9ddd-4f077264d224",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Compare two tokens\n",
        "doc = nlp(\"I like pizza and pasta\")\n",
        "token1 = doc[2]\n",
        "token2 = doc[4]\n",
        "print(token1.similarity(token2))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.7369546\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ATimRoh_l19",
        "colab_type": "text"
      },
      "source": [
        "**Similarity examples**\n",
        "\n",
        "- the similarity methods to compare different types of objects.\n",
        "\n",
        "For example, a document and a token.\n",
        "\n",
        "Here, the similarity score is pretty low and the two objects are considered fairly dissimilar.\n",
        "\n",
        "Here's another example comparing a span – \"pizza and pasta\" – to a document about McDonalds.\n",
        "\n",
        "The score returned here is 0.61, so it's determined to be kind of similar."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hhiGf3m2_kJP",
        "colab_type": "code",
        "outputId": "1d0d3248-7413-49fe-f5aa-52874f3ffd5a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Compare a document with a token\n",
        "doc = nlp(\"I like pizza\")\n",
        "token = nlp(\"soap\")[0]\n",
        "\n",
        "print(doc.similarity(token))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.32531983166759537\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dlvLoM1mAFVp",
        "colab_type": "code",
        "outputId": "ee792b31-9592-42ba-999f-f8f2e15e6030",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Compare a span with a document\n",
        "span = nlp(\"I like pizza and pasta\")[2:5]\n",
        "doc = nlp(\"McDonalds sells burgers\")\n",
        "\n",
        "print(span.similarity(doc))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.6199092090831612\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nWTjNjP5AVDD",
        "colab_type": "text"
      },
      "source": [
        "**How does spaCy predict similarity?**\n",
        "- Similarity is determined using word vectors\n",
        "- Multi-dimensional meaning representations of words\n",
        "- Generated using an algorithm like Word2Vec and lots of text\n",
        "- Can be added to spaCy's statistical models\n",
        "      Default: cosine similarity, but can be adjusted\n",
        "- Doc and Span vectors default to average of token vectors\n",
        "- Short phrases are better than long documents with many irrelevant words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z5k6y6z8A14A",
        "colab_type": "text"
      },
      "source": [
        "**how does spaCy do this under the hood?**\n",
        "\n",
        "1. Similarity is determined using word vectors, multi-dimensional representations of meanings of words.\n",
        "\n",
        "2. You might have heard of Word2Vec, which is an algorithm that's often used to train word vectors from raw text.\n",
        "\n",
        "3. Vectors can be added to spaCy's statistical models.\n",
        "\n",
        "4. By default, the similarity returned by spaCy is the cosine similarity between two vectors – but this can be adjusted if necessary.\n",
        "\n",
        "5. Vectors for objects consisting of several tokens, like the Doc and Span, default to the average of their token vectors.\n",
        "\n",
        "That's also why you usually get more value out of shorter phrases with fewer irrelevant words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-JicyqNNAURu",
        "colab_type": "code",
        "outputId": "76401de1-6b3c-46cc-cd39-5d7429fb5316",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 867
        }
      },
      "source": [
        "# Load a larger model with vectors\n",
        "nlp = spacy.load('en_core_web_lg')\n",
        "\n",
        "doc = nlp(\"I have a banana\")\n",
        "# Access the vector via the token.vector attribute\n",
        "print(doc[3].vector)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 2.0228e-01 -7.6618e-02  3.7032e-01  3.2845e-02 -4.1957e-01  7.2069e-02\n",
            " -3.7476e-01  5.7460e-02 -1.2401e-02  5.2949e-01 -5.2380e-01 -1.9771e-01\n",
            " -3.4147e-01  5.3317e-01 -2.5331e-02  1.7380e-01  1.6772e-01  8.3984e-01\n",
            "  5.5107e-02  1.0547e-01  3.7872e-01  2.4275e-01  1.4745e-02  5.5951e-01\n",
            "  1.2521e-01 -6.7596e-01  3.5842e-01 -4.0028e-02  9.5949e-02 -5.0690e-01\n",
            " -8.5318e-02  1.7980e-01  3.3867e-01  1.3230e-01  3.1021e-01  2.1878e-01\n",
            "  1.6853e-01  1.9874e-01 -5.7385e-01 -1.0649e-01  2.6669e-01  1.2838e-01\n",
            " -1.2803e-01 -1.3284e-01  1.2657e-01  8.6723e-01  9.6721e-02  4.8306e-01\n",
            "  2.1271e-01 -5.4990e-02 -8.2425e-02  2.2408e-01  2.3975e-01 -6.2260e-02\n",
            "  6.2194e-01 -5.9900e-01  4.3201e-01  2.8143e-01  3.3842e-02 -4.8815e-01\n",
            " -2.1359e-01  2.7401e-01  2.4095e-01  4.5950e-01 -1.8605e-01 -1.0497e+00\n",
            " -9.7305e-02 -1.8908e-01 -7.0929e-01  4.0195e-01 -1.8768e-01  5.1687e-01\n",
            "  1.2520e-01  8.4150e-01  1.2097e-01  8.8239e-02 -2.9196e-02  1.2151e-03\n",
            "  5.6825e-02 -2.7421e-01  2.5564e-01  6.9793e-02 -2.2258e-01 -3.6006e-01\n",
            " -2.2402e-01 -5.3699e-02  1.2022e+00  5.4535e-01 -5.7998e-01  1.0905e-01\n",
            "  4.2167e-01  2.0662e-01  1.2936e-01 -4.1457e-02 -6.6777e-01  4.0467e-01\n",
            " -1.5218e-02 -2.7640e-01 -1.5611e-01 -7.9198e-02  4.0037e-02 -1.2944e-01\n",
            " -2.4090e-04 -2.6785e-01 -3.8115e-01 -9.7245e-01  3.1726e-01 -4.3951e-01\n",
            "  4.1934e-01  1.8353e-01 -1.5260e-01 -1.0808e-01 -1.0358e+00  7.6217e-02\n",
            "  1.6519e-01  2.6526e-04  1.6616e-01 -1.5281e-01  1.8123e-01  7.0274e-01\n",
            "  5.7956e-03  5.1664e-02 -5.9745e-02 -2.7551e-01 -3.9049e-01  6.1132e-02\n",
            "  5.5430e-01 -8.7997e-02 -4.1681e-01  3.2826e-01 -5.2549e-01 -4.4288e-01\n",
            "  8.2183e-03  2.4486e-01 -2.2982e-01 -3.4981e-01  2.6894e-01  3.9166e-01\n",
            " -4.1904e-01  1.6191e-01 -2.6263e+00  6.4134e-01  3.9743e-01 -1.2868e-01\n",
            " -3.1946e-01 -2.5633e-01 -1.2220e-01  3.2275e-01 -7.9933e-02 -1.5348e-01\n",
            "  3.1505e-01  3.0591e-01  2.6012e-01  1.8553e-01 -2.4043e-01  4.2886e-02\n",
            "  4.0622e-01 -2.4256e-01  6.3870e-01  6.9983e-01 -1.4043e-01  2.5209e-01\n",
            "  4.8984e-01 -6.1067e-02 -3.6766e-01 -5.5089e-01 -3.8265e-01 -2.0843e-01\n",
            "  2.2832e-01  5.1218e-01  2.7868e-01  4.7652e-01  4.7951e-02 -3.4008e-01\n",
            " -3.2873e-01 -4.1967e-01 -7.5499e-02 -3.8954e-01 -2.9622e-02 -3.4070e-01\n",
            "  2.2170e-01 -6.2856e-02 -5.1903e-01 -3.7774e-01 -4.3477e-03 -5.8301e-01\n",
            " -8.7546e-02 -2.3929e-01 -2.4711e-01 -2.5887e-01 -2.9894e-01  1.3715e-01\n",
            "  2.9892e-02  3.6544e-02 -4.9665e-01 -1.8160e-01  5.2939e-01  2.1992e-01\n",
            " -4.4514e-01  3.7798e-01 -5.7062e-01 -4.6946e-02  8.1806e-02  1.9279e-02\n",
            "  3.3246e-01 -1.4620e-01  1.7156e-01  3.9981e-01  3.6217e-01  1.2816e-01\n",
            "  3.1644e-01  3.7569e-01 -7.4690e-02 -4.8480e-02 -3.1401e-01 -1.9286e-01\n",
            " -3.1294e-01 -1.7553e-02 -1.7514e-01 -2.7587e-02 -1.0000e+00  1.8387e-01\n",
            "  8.1434e-01 -1.8913e-01  5.0999e-01 -9.1960e-03 -1.9295e-03  2.8189e-01\n",
            "  2.7247e-02  4.3409e-01 -5.4967e-01 -9.7426e-02 -2.4540e-01 -1.7203e-01\n",
            " -8.8650e-02 -3.0298e-01 -1.3591e-01 -2.7765e-01  3.1286e-03  2.0556e-01\n",
            " -1.5772e-01 -5.2308e-01 -6.4701e-01 -3.7014e-01  6.9393e-02  1.1401e-01\n",
            "  2.7594e-01 -1.3875e-01 -2.7268e-01  6.6891e-01 -5.6454e-02  2.4017e-01\n",
            " -2.6730e-01  2.9860e-01  1.0083e-01  5.5592e-01  3.2849e-01  7.6858e-02\n",
            "  1.5528e-01  2.5636e-01 -1.0772e-01 -1.2359e-01  1.1827e-01 -9.9029e-02\n",
            " -3.4328e-01  1.1502e-01 -3.7808e-01 -3.9012e-02 -3.4593e-01 -1.9404e-01\n",
            " -3.3580e-01 -6.2334e-02  2.8919e-01  2.8032e-01 -5.3741e-01  6.2794e-01\n",
            "  5.6955e-02  6.2147e-01 -2.5282e-01  4.1670e-01 -1.0108e-02 -2.5434e-01\n",
            "  4.0003e-01  4.2432e-01  2.2672e-01  1.7553e-01  2.3049e-01  2.8323e-01\n",
            "  1.3882e-01  3.1218e-03  1.7057e-01  3.6685e-01  2.5247e-03 -6.4009e-01\n",
            " -2.9765e-01  7.8943e-01  3.3168e-01 -1.1966e+00 -4.7156e-02  5.3175e-01]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OdmM6464BJMg",
        "colab_type": "text"
      },
      "source": [
        "you an idea of what those vectors look like, here's an example.\n",
        "\n",
        "First, we load the medium model again, which ships with word vectors.\n",
        "\n",
        "Next, we can process a text and look up a token's vector using the dot vector attribute.\n",
        "\n",
        "The result is a 300-dimensional vector of the word \"banana\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3f4OxA8tBPY5",
        "colab_type": "text"
      },
      "source": [
        "**Similarity depends on the application context**\n",
        "\n",
        "- Predicting similarity can be useful for many types of applications. For example, to recommend a user similar texts based on the ones they have read. It can also be helpful to flag duplicate content, like posts on an online platform.\n",
        "\n",
        "- However, it's important to keep in mind that there's no objective definition of what's similar and what isn't. It always depends on the context and what your application needs to do.\n",
        "\n",
        "- **Here's an example:** spaCy's default word vectors assign a very high similarity score to \"I like cats\" and \"I hate cats\". This makes sense, because both texts express sentiment about cats. But in a different application context, you might want to consider the phrases as very dissimilar, because they talk about opposite sentiments."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cd8iomLkBK7H",
        "colab_type": "code",
        "outputId": "28abdf08-8da7-48dd-aef5-cc2451a82178",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "doc1 = nlp(\"I like cats\")\n",
        "doc2 = nlp(\"I hate cats\")\n",
        "\n",
        "print(doc1.similarity(doc2))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9501447503553421\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a6DkLTVgCUOt",
        "colab_type": "code",
        "outputId": "7a3786c4-e6fe-4452-fd00-a4db5946b27e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 867
        }
      },
      "source": [
        "import spacy\n",
        "# !python -m spacy download en_core_web_md\n",
        "# Load the en_core_web_md model\n",
        "nlp = spacy.load(\"en_core_web_md\")\n",
        "\n",
        "# Process a text\n",
        "doc = nlp(\"Two bananas in pyjamas\")\n",
        "\n",
        "# Get the vector for the token \"bananas\"\n",
        "bananas_vector = doc[1].vector\n",
        "print(bananas_vector)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[-2.2009e-01 -3.0322e-02 -7.9859e-02 -4.6279e-01 -3.8600e-01  3.6962e-01\n",
            " -7.7178e-01 -1.1529e-01  3.3601e-02  5.6573e-01 -2.4001e-01  4.1833e-01\n",
            "  1.5049e-01  3.5621e-01 -2.1508e-01 -4.2743e-01  8.1400e-02  3.3916e-01\n",
            "  2.1637e-01  1.4792e-01  4.5811e-01  2.0966e-01 -3.5706e-01  2.3800e-01\n",
            "  2.7971e-02 -8.4538e-01  4.1917e-01 -3.9181e-01  4.0434e-04 -1.0662e+00\n",
            "  1.4591e-01  1.4643e-03  5.1277e-01  2.6072e-01  8.3785e-02  3.0340e-01\n",
            "  1.8579e-01  5.9999e-02 -4.0270e-01  5.0888e-01 -1.1358e-01 -2.8854e-01\n",
            " -2.7068e-01  1.1017e-02 -2.2217e-01  6.9076e-01  3.6459e-02  3.0394e-01\n",
            "  5.6989e-02  2.2733e-01 -9.9473e-02  1.5165e-01  1.3540e-01 -2.4965e-01\n",
            "  9.8078e-01 -8.0492e-01  1.9326e-01  3.1128e-01  5.5390e-02 -4.2423e-01\n",
            " -1.4082e-02  1.2708e-01  1.8868e-01  5.9777e-02 -2.2215e-01 -8.3950e-01\n",
            "  9.1987e-02  1.0180e-01 -3.1299e-01  5.5083e-01 -3.0717e-01  4.4201e-01\n",
            "  1.2666e-01  3.7643e-01  3.2333e-01  9.5673e-02  2.5083e-01 -6.4049e-02\n",
            "  4.2143e-01 -1.9375e-01  3.8026e-01  7.0883e-03 -2.0371e-01  1.5402e-01\n",
            " -3.7877e-03 -2.9396e-01  9.6518e-01  2.0068e-01 -5.6572e-01 -2.2581e-01\n",
            "  3.2251e-01 -3.4634e-01  2.7064e-01 -2.0687e-01 -4.7229e-01  3.1704e-01\n",
            " -3.4665e-01 -2.5188e-01 -1.1201e-01 -3.3937e-01  3.1518e-01 -3.2221e-01\n",
            " -2.4530e-01 -7.1571e-02 -4.3971e-01 -1.2070e+00  3.3365e-01 -5.8208e-02\n",
            "  8.0899e-01  4.2335e-01  3.8678e-01 -6.0797e-01 -7.3760e-01 -2.0547e-01\n",
            " -1.7499e-01 -3.7842e-03  2.1930e-01 -5.2486e-02  3.4869e-01  4.3852e-01\n",
            " -3.4471e-01  2.8910e-01  7.2554e-02 -4.8625e-01 -3.8390e-01 -4.4760e-01\n",
            "  4.3278e-01 -2.7128e-03 -9.0067e-01 -3.0819e-02 -3.8630e-01 -8.0798e-02\n",
            " -1.6243e-01  2.8830e-01 -2.6349e-01  1.7628e-01  3.5958e-01  5.7672e-01\n",
            " -5.4624e-01  3.8555e-02 -2.0182e+00  3.2916e-01  3.4672e-01  1.5398e-01\n",
            " -4.3446e-01 -4.1428e-02 -6.9588e-02  5.1513e-01 -1.3489e-01 -5.7239e-02\n",
            "  4.9241e-01  1.8643e-01  3.8596e-01 -3.7329e-02 -5.4216e-01 -1.8152e-01\n",
            "  4.3110e-01 -4.6967e-01  6.6801e-02  5.0323e-01 -2.4059e-01  3.6742e-01\n",
            "  2.9300e-01 -8.7883e-02 -4.7940e-01 -4.3431e-02 -2.6137e-01 -6.2658e-01\n",
            "  1.1446e-01  2.7682e-01  3.4800e-01  5.0018e-01  1.4269e-01 -3.3545e-01\n",
            " -3.9712e-01 -3.3121e-01 -3.4434e-01 -4.1627e-01 -3.5707e-03 -6.2350e-01\n",
            "  3.7794e-01 -1.6765e-01 -4.1954e-01 -3.3134e-01  3.1232e-01 -3.9494e-01\n",
            " -4.6921e-03 -4.8884e-01 -2.2059e-02 -2.6174e-01  1.7937e-01  3.6628e-01\n",
            "  5.8971e-02 -3.5991e-01 -4.4393e-01 -1.1890e-01  3.3487e-01  3.6505e-02\n",
            " -3.2788e-01  3.3425e-01 -5.6361e-01 -1.1190e-01  5.3770e-01  2.0311e-01\n",
            "  1.5110e-01  1.0623e-02  3.3401e-01  4.6084e-01  5.6293e-01 -7.5432e-02\n",
            "  5.4813e-01  1.9395e-01 -2.6265e-01 -3.1699e-01 -8.1778e-01  5.8169e-02\n",
            " -5.7866e-02 -1.1781e-01 -5.8742e-02 -1.4092e-01 -9.9394e-01 -9.4532e-02\n",
            "  2.3503e-01 -4.9027e-01  8.5832e-01  1.1540e-01 -1.5049e-01  1.9065e-01\n",
            " -2.6705e-01  2.5326e-01 -6.7579e-01 -1.0633e-02 -5.5158e-02 -3.1004e-01\n",
            " -5.8036e-02 -1.7200e-01  1.3298e-01 -3.2899e-01 -7.5481e-02  2.9425e-02\n",
            " -3.2949e-01 -1.8691e-01 -9.5323e-01 -3.5468e-01 -3.3162e-01  5.6441e-02\n",
            "  2.1790e-02  1.7182e-01 -4.4267e-01  6.9765e-01 -2.6876e-01  1.1659e-01\n",
            " -1.6584e-01  3.8296e-01  2.9109e-01  3.6318e-01  3.6961e-01  1.6305e-01\n",
            "  1.8152e-01  2.2453e-01  3.9866e-02 -3.7607e-02 -3.6089e-01  7.0818e-02\n",
            " -2.1509e-01  3.6551e-01 -5.1603e-01 -5.8102e-03 -4.8320e-01 -2.5068e-01\n",
            " -5.2062e-02 -2.0828e-01  2.9060e-01  2.2084e-02 -6.8123e-01  4.2063e-01\n",
            "  9.5973e-02  8.1720e-01 -1.5241e-01  6.2994e-01  2.6449e-01 -1.3516e-01\n",
            "  3.2450e-01  3.0503e-01  1.2357e-01  1.5107e-01  2.8327e-01 -3.3838e-01\n",
            "  4.6106e-02 -1.2361e-01  1.4516e-01 -2.7947e-02  2.6231e-02 -5.9591e-01\n",
            " -4.4183e-01  7.8440e-01 -3.4375e-02 -1.3928e+00  3.5248e-01  6.5220e-01]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SjvoMYywDLY2",
        "colab_type": "code",
        "outputId": "1211fd5a-ca6c-4e32-ce46-d760f7af762a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "doc1 = nlp(\"It's a warm summer day\")\n",
        "doc2 = nlp(\"It's sunny outside\")\n",
        "\n",
        "# Get the similarity of doc1 and doc2\n",
        "similarity = doc1.similarity(doc2)\n",
        "print(similarity)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.8789265574516525\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6TowzT8DDXH9",
        "colab_type": "code",
        "outputId": "362c97e1-00c0-418e-87f1-c7fc5c1dbf50",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "doc = nlp(\"TV and books\")\n",
        "token1, token2 = doc[0], doc[2]\n",
        "\n",
        "# Get the similarity of the tokens \"TV\" and \"books\"\n",
        "similarity = token1.similarity(token2)\n",
        "print(similarity)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.22325331\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NAcA4XwtDdwE",
        "colab_type": "code",
        "outputId": "2271483c-719c-4a40-e859-fc80cc6792e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "doc = nlp(\"This was a great restaurant. Afterwards, we went to a really nice bar.\")\n",
        "\n",
        "# Create spans for \"great restaurant\" and \"really nice bar\"\n",
        "span1 = doc[3:5]\n",
        "span2 = doc[12:15]\n",
        "\n",
        "# Get the similarity of the spans\n",
        "similarity = span1.similarity(span2)\n",
        "print(similarity)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.75173926\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OXcLvdiJDaEv",
        "colab_type": "text"
      },
      "source": [
        "**Combining models and rules**\n",
        "Combining statistical models with rule-based systems is one of the most powerful tricks you should have in your NLP toolbox.\n",
        "\n",
        "**Statistical predictions vs. rules**\n",
        "\n",
        "- Statistical models are useful if your application needs to be able to generalize based on a few examples.\n",
        "\n",
        "- For instance, detecting product or person names usually benefits from a statistical model. Instead of providing a list of all person names ever, your application will be able to predict whether a span of tokens is a person name. Similarly, you can predict dependency labels to find subject/object relationships.\n",
        "\n",
        "- To do this, you would use spaCy's entity recognizer, dependency parser or part-of-speech tagger.\n",
        "\n",
        "---\n",
        "\n",
        "Rule-based approaches on the other hand come in handy if there's a more or less finite number of instances you want to find. For example, all countries or cities of the world, drug names or even dog breeds.\n",
        "\n",
        "In spaCy, you can achieve this with custom tokenization rules, as well as the matcher and phrase matcher.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cahU2bRXEmpM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Initialize with the shared vocab\n",
        "from spacy.matcher import Matcher\n",
        "matcher = Matcher(nlp.vocab)\n",
        "\n",
        "# Patterns are lists of dictionaries describing the tokens\n",
        "pattern = [{'LEMMA': 'love', 'POS': 'VERB'}, {'LOWER': 'cats'}]\n",
        "matcher.add('LOVE_CATS', None, pattern)\n",
        "\n",
        "# Operators can specify how often a token should be matched\n",
        "pattern = [{'TEXT': 'very', 'OP': '+'}, {'TEXT': 'happy'}]\n",
        "\n",
        "# Calling matcher on doc returns list of (match_id, start, end) tuples\n",
        "doc = nlp(\"I love cats and I'm very very happy\")\n",
        "matches = matcher(doc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c0Xpwr-wGF5C",
        "colab_type": "code",
        "outputId": "8a221667-0f5c-4276-947b-db3d43575b58",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "doc,matches"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(I love cats and I'm very very happy, [(9137535031263442622, 1, 3)])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fP-2Pht4GPOk",
        "colab_type": "text"
      },
      "source": [
        "spaCy's rule-based matcher to find complex patterns in your texts. Here's a quick recap.\n",
        "\n",
        "The matcher is initialized with the shared vocabulary – usually nlp dot vocab.\n",
        "\n",
        "Patterns are lists of dictionaries, and each dictionary describes one token and its attributes. Patterns can be added to the matcher using the matcher dot add method.\n",
        "\n",
        "Operators let you specify how often to match a token. For example, \"+\" will match one or more times.\n",
        "\n",
        "Calling the matcher on a doc object will return a list of the matches. Each match is a tuple consisting of an ID, and the start and end token index in the document."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P21XJLpnGR7J",
        "colab_type": "code",
        "outputId": "c46bb43b-5a82-4110-9a4e-85ddd822f45f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "matcher = Matcher(nlp.vocab)\n",
        "matcher.add('DOG', None, [{'LOWER': 'golden'}, {'LOWER': 'retriever'}])\n",
        "doc = nlp(\"I have a Golden Retriever\")\n",
        "\n",
        "for match_id, start, end in matcher(doc):\n",
        "    span = doc[start:end]\n",
        "    print('Matched span:', span.text)\n",
        "    # Get the span's root token and root head token\n",
        "    print('Root token:', span.root.text)\n",
        "    print('Root head token:', span.root.head.text)\n",
        "    # Get the previous token and its POS tag\n",
        "    print('Previous token:', doc[start - 1].text, doc[start - 1].pos_)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Matched span: Golden Retriever\n",
            "Root token: Retriever\n",
            "Root head token: have\n",
            "Previous token: a DET\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZKBeVGHXGbHh",
        "colab_type": "text"
      },
      "source": [
        "matcher rule for \"golden retriever\".\n",
        "\n",
        "If we iterate over the matches returned by the matcher, we can get the match ID and the start and end index of the matched span. We can then find out more about it. Span objects give us access to the original document and all other token attributes and linguistic features predicted by the model.\n",
        "\n",
        "For example, we can get the span's root token. If the span consists of more than one token, this will be the token that decides the category of the phrase. For example, the root of \"Golden Retriever\" is \"Retriever\". We can also find the head token of the root. This is the syntactic \"parent\" that governs the phrase – in this case, the verb \"have\".\n",
        "\n",
        "Finally, we can look at the previous token and its attributes. In this case, it's a determiner, the article \"a\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-VchMQ8FGgP6",
        "colab_type": "text"
      },
      "source": [
        "**Efficient phrase matching**\n",
        "- PhraseMatcher like regular expressions or keyword search – but with access to the tokens!\n",
        "- Takes Doc object as patterns\n",
        "- More efficient and faster than the Matcher\n",
        "- Great for matching large word list"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W6JPIIMeGc7c",
        "colab_type": "code",
        "outputId": "6b091878-bcbc-4e8f-e29b-c28a0e2b3fb9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from spacy.matcher import PhraseMatcher\n",
        "\n",
        "matcher = PhraseMatcher(nlp.vocab)\n",
        "\n",
        "pattern = nlp(\"Golden Retriever\")\n",
        "matcher.add('DOG', None, pattern)\n",
        "doc = nlp(\"I have a Golden Retriever\")\n",
        "\n",
        "# Iterate over the matches\n",
        "for match_id, start, end in matcher(doc):\n",
        "    # Get the matched span\n",
        "    span = doc[start:end]\n",
        "    print('Matched span:', span.text)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Matched span: Golden Retriever\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yhyn1bf1GstY",
        "colab_type": "text"
      },
      "source": [
        "The phrase matcher can be imported from spacy dot matcher and follows the same API as the regular matcher.\n",
        "\n",
        "Instead of a list of dictionaries, we pass in a Doc object as the pattern.\n",
        "\n",
        "We can then iterate over the matches in the text, which gives us the match ID, and the start and end of the match. This lets us create a Span object for the matched tokens \"Golden Retriever\" to analyze it in context."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dWrlTcqRG0Ey",
        "colab_type": "text"
      },
      "source": [
        "**Why does this pattern not match the tokens “Silicon Valley” in the doc?**\n",
        "\n",
        "**Answer:** The tokenizer doesn't create tokens for single spaces, so there's no token with the value ' ' in between.\n",
        "\n",
        "**Reason:** The tokenizer already takes care of splitting off whitespace and each dictionary in the pattern describes one token.\n",
        "\n",
        "By default, all tokens described by a pattern will be matched exactly once. Operators are only needed to change this behavior – for example, to match zero or more times."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7qN4kyjoGu2z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(\n",
        "    \"Twitch Prime, the perks program for Amazon Prime members offering free \"\n",
        "    \"loot, games and other benefits, is ditching one of its best features: \"\n",
        "    \"ad-free viewing. According to an email sent out to Amazon Prime members \"\n",
        "    \"today, ad-free viewing will no longer be included as a part of Twitch \"\n",
        "    \"Prime for new members, beginning on September 14. However, members with \"\n",
        "    \"existing annual subscriptions will be able to continue to enjoy ad-free \"\n",
        "    \"viewing until their subscription comes up for renewal. Those with \"\n",
        "    \"monthly subscriptions will have access to ad-free viewing until October 15.\"\n",
        ")\n",
        "\n",
        "# Create the match patterns\n",
        "pattern1 = [{\"LOWER\": \"Amazon\"}, {\"IS_TITLE\": True, \"POS\": \"PROPN\"}]\n",
        "pattern2 = [{\"LOWER\": \"ad-free\"}, {\"POS\": \"NOUN\"}]\n",
        "\n",
        "# Initialize the Matcher and add the patterns\n",
        "matcher = Matcher(nlp.vocab)\n",
        "matcher.add(\"PATTERN1\", None, pattern1)\n",
        "matcher.add(\"PATTERN2\", None, pattern2)\n",
        "\n",
        "# Iterate over the matches\n",
        "for match_id, start, end in matcher(doc):\n",
        "    # Print pattern string name and text of matched span\n",
        "    print(doc.vocab.strings[match_id], doc[start:end].text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xFaepah1Hb-U",
        "colab_type": "text"
      },
      "source": [
        "Sometimes it’s more efficient to match exact strings instead of writing patterns describing the individual tokens. This is especially true for finite categories of things – like all countries of the world. We already have a list of countries, so let’s use this as the basis of our information extraction script. A list of string names is available as the variable COUNTRIES.\n",
        "\n",
        "Import the PhraseMatcher and initialize it with the shared vocab as the variable matcher.\n",
        "Add the phrase patterns and call the matcher on the doc."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9xLCr40dHpIo",
        "colab_type": "code",
        "outputId": "733a791b-896c-4d8b-cfd1-274a1c5fcd5d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        }
      },
      "source": [
        "import json\n",
        "from spacy.lang.en import English\n",
        "\n",
        "with open(\"exercises/countries.json\") as f:\n",
        "    COUNTRIES = json.loads(f.read())\n",
        "\n",
        "nlp = English()\n",
        "doc = nlp(\"Czech Republic may help Slovakia protect its airspace\")\n",
        "\n",
        "# Import the PhraseMatcher and initialize it\n",
        "from spacy.matcher import PhraseMatcher\n",
        "\n",
        "matcher = PhraseMatcher(nlp.vocab)\n",
        "\n",
        "# Create pattern Doc objects and add them to the matcher\n",
        "# This is the faster version of: [nlp(country) for country in COUNTRIES]\n",
        "patterns = list(nlp.pipe(COUNTRIES))\n",
        "matcher.add(\"COUNTRY\", None, *patterns)\n",
        "\n",
        "# Call the matcher on the test document and print the result\n",
        "matches = matcher(doc)\n",
        "print([doc[start:end] for match_id, start, end in matches])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-5c3523f96700>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0men\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEnglish\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"exercises/countries.json\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mCOUNTRIES\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'exercises/countries.json'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PAPYSXsLH4l4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from spacy.lang.en import English\n",
        "from spacy.matcher import PhraseMatcher\n",
        "from spacy.tokens import Span\n",
        "import json\n",
        "\n",
        "with open(\"exercises/countries.json\") as f:\n",
        "    COUNTRIES = json.loads(f.read())\n",
        "with open(\"exercises/country_text.txt\") as f:\n",
        "    TEXT = f.read()\n",
        "\n",
        "nlp = English()\n",
        "matcher = PhraseMatcher(nlp.vocab)\n",
        "patterns = list(nlp.pipe(COUNTRIES))\n",
        "matcher.add(\"COUNTRY\", None, *patterns)\n",
        "\n",
        "# Create a doc and find matches in it\n",
        "doc = nlp(TEXT)\n",
        "\n",
        "# Iterate over the matches\n",
        "for match_id, start, end in matcher(doc):\n",
        "    # Create a Span with the label for \"GPE\"\n",
        "    span = Span(doc, start, end, label=\"GPE\")\n",
        "\n",
        "    # Overwrite the doc.ents and add the span\n",
        "    doc.ents = list(doc.ents) + [span]\n",
        "\n",
        "    # Get the span's root head token\n",
        "    span_root_head = span.root.head\n",
        "    # Print the text of the span root's head token and the span text\n",
        "    print(span_root_head.text, \"-->\", span.text)\n",
        "\n",
        "# Print the entities in the document\n",
        "print([(ent.text, ent.label_) for ent in doc.ents if ent.label_ == \"GPE\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fUjP0bq1KKR-",
        "colab_type": "text"
      },
      "source": [
        "# Chapter 3: Processing Pipelines\n",
        "\n",
        "A series of functions applied to a Doc to add attributes like part-of-speech tags, dependency labels or named entities"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GbgCCNZZC6H_",
        "colab_type": "text"
      },
      "source": [
        "`spacy.load()` to load a model, spaCy will initialize the language, add the pipeline and load in the binary model weights. When you call the nlp object on a text, the model is already loaded.\n",
        "\n",
        "Tokenizer always run before all other pipeline components, because it transforms a string of text into a Doc object. The pipeline also doesn’t have to consist of the tagger, parser and entity recognizer.\n",
        "\n",
        "`doc = nlp(\"This is a sentence.\")` - Tokenize the text and apply each pipeline component in order."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DMb7ssjSCpRW",
        "colab_type": "code",
        "outputId": "b62d05d2-7cb6-4aa0-9f68-84712cc1c5d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "# Load the en_core_web_sm model\n",
        "nlp = spacy.load('en_core_web_sm')        #Tokenize the text and apply each pipeline component in order\n",
        "\n",
        "# Print the names of the pipeline components\n",
        "print(nlp.pipe_names)\n",
        "\n",
        "# Print the full pipeline of (name, component) tuples\n",
        "print(nlp.pipeline)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['tagger', 'parser', 'ner']\n",
            "[('tagger', <spacy.pipeline.pipes.Tagger object at 0x7fc6b64471d0>), ('parser', <spacy.pipeline.pipes.DependencyParser object at 0x7fc6b65505e8>), ('ner', <spacy.pipeline.pipes.EntityRecognizer object at 0x7fc6b6550588>)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55MRSNQkEwhr",
        "colab_type": "text"
      },
      "source": [
        "### Custom pipeline components\n",
        "\n",
        "let's us add our own function to the spaCy pipeline that is executed when we call the nlp object on a text – for example, to modify the Doc and add more data to it.\n",
        "\n",
        "![alt text](https://course.spacy.io/pipeline.png)\n",
        "\n",
        "why custom components?\n",
        "- Make a function execute automatically when you call nlp\n",
        "- Adds our own metadata to documents and tokens\n",
        "- Updating built-in attributes like `doc.ents`\n",
        "\n",
        "Custom components are executed automatically when you call the nlp object on a text.\n",
        "\n",
        "They're especially useful for adding your own custom metadata to documents and tokens.\n",
        "\n",
        "we can also use them to update built-in attributes, like the named entity spans."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QpDhuRRzGX15",
        "colab_type": "text"
      },
      "source": [
        "> Pipeline component is a function or callable that takes a doc, modifies it and returns it, so it can be processed by the next component in the pipeline.\n",
        "\n",
        "> Components can be added to the pipeline using the `nlp.add_pipe` method. The method takes at least one argument: the component function.\n",
        "\n",
        "\n",
        "    def custom_component(doc):\n",
        "      # Do something to the doc here\n",
        "      return doc\n",
        "\n",
        "    nlp.add_pipe(custom_component)\n",
        "\n",
        "|Argument|\tDescription|\tExample|\n",
        "|---|---|---|\n",
        "|last\t|If True, add last|\tnlp.add_pipe(component, last=True)|\n",
        "|first |\tIf True, add first|\tnlp.add_pipe(component, first=True)|\n",
        "|before |\tAdd before component|\tnlp.add_pipe(component, before='ner')|\n",
        "|after |\tAdd after component|\tnlp.add_pipe(component, after='tagger')|\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ePozp_3QErQ2",
        "colab_type": "code",
        "outputId": "fda81612-2438-4e26-d7f0-5fd0998009cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import spacy\n",
        "# Create the nlp object\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Define a custom component\n",
        "def custom_component(doc):\n",
        "    # Print the doc's length\n",
        "    print('Doc length:', len(doc))\n",
        "    # Return the doc object\n",
        "    return doc\n",
        "\n",
        "# Add the component first in the pipeline\n",
        "nlp.add_pipe(custom_component, first=True)\n",
        "\n",
        "# Print the pipeline component names\n",
        "print('Pipeline:', nlp.pipe_names)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Pipeline: ['custom_component', 'tagger', 'parser', 'ner']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MOFBsA5WH5uw",
        "colab_type": "code",
        "outputId": "63fb743e-44ad-4ebd-bf03-027ba8d30297",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Process a text\n",
        "doc = nlp(\"Hello world!\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Doc length: 3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0sWPsRtLH9mP",
        "colab_type": "code",
        "outputId": "c5f24615-1cc1-45fc-930b-eee02ed77605",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "doc"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Hello world!"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lCuYb4llIbyu",
        "colab_type": "text"
      },
      "source": [
        "**Which of these problems can be solved by custom pipeline components? Choose all that apply!**\n",
        "\n",
        "1. Updating the pre-trained models and improving their predictions\n",
        "2. Computing your own values based on tokens and their attributes\n",
        "3. Adding named entities, for example based on a dictionary\n",
        "4. Implementing support for an additional language\n",
        "\n",
        "**ANS:** 2 & 3\n",
        "\n",
        "**Reason:** \n",
        "1. Custom components can only modify the Doc and can’t be used to update weights of other components directly.  \n",
        "2. Great for adding custom values to documents, tokens and spans, and customizing the `doc.ents`. \n",
        "3. Custom components are added to the pipeline after the language class is already initialized and after tokenization, so they’re not suitable to add new languages."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sRPwx3dbJpia",
        "colab_type": "code",
        "outputId": "934e4389-57a3-47a7-f4e1-de04f9f16e5e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Define the custom component\n",
        "def length_component(doc):\n",
        "    # Get the doc's length\n",
        "    doc_length = len(doc)\n",
        "    print(\"This document is {} tokens long.\".format(doc_length))\n",
        "    # Return the doc\n",
        "    return doc\n",
        "\n",
        "\n",
        "# Load the small English model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Add the component first in the pipeline and print the pipe names\n",
        "nlp.add_pipe(length_component, first=True)\n",
        "print(nlp.pipe_names)\n",
        "\n",
        "# Process a text\n",
        "doc = nlp(\"This is a sentence.\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['length_component', 'tagger', 'parser', 'ner']\n",
            "This document is 5 tokens long.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4x0kjgJuJ_ko",
        "colab_type": "code",
        "outputId": "13fce9b7-b97c-483e-c427-356551867978",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# The PhraseMatcher to find animal names in the document and adds the matched spans to the `doc.ents`. \n",
        "# A PhraseMatcher with the animal patterns has already been created as the variable matcher.\n",
        "\n",
        "from spacy.matcher import PhraseMatcher\n",
        "from spacy.tokens import Span\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "animals = [\"Golden Retriever\", \"cat\", \"turtle\", \"Rattus norvegicus\"]\n",
        "animal_patterns = list(nlp.pipe(animals))\n",
        "print(\"animal_patterns:\", animal_patterns)\n",
        "matcher = PhraseMatcher(nlp.vocab)\n",
        "matcher.add(\"ANIMAL\", None, *animal_patterns)\n",
        "\n",
        "# Define the custom component\n",
        "def animal_component(doc):\n",
        "    # Apply the matcher to the doc\n",
        "    matches = matcher(doc)\n",
        "    # Create a Span for each match and assign the label 'ANIMAL'\n",
        "    spans = [Span(doc, start, end, label='ANIMAL') for match_id, start, end in matches]\n",
        "    # Overwrite the doc.ents with the matched spans\n",
        "    doc.ents = spans\n",
        "    return doc\n",
        "\n",
        "\n",
        "# Add the component to the pipeline after the 'ner' component\n",
        "nlp.add_pipe(animal_component, after='ner')\n",
        "print(nlp.pipe_names)\n",
        "\n",
        "# Process the text and print the text and label for the doc.ents\n",
        "doc = nlp(\"I have a cat and a Golden Retriever\")\n",
        "print([(ent.text, ent.label_) for ent in doc.ents])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "animal_patterns: [Golden Retriever, cat, turtle, Rattus norvegicus]\n",
            "['tagger', 'parser', 'ner', 'animal_component']\n",
            "[('cat', 'ANIMAL'), ('Golden Retriever', 'ANIMAL')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qwp-rDxLM6kq",
        "colab_type": "text"
      },
      "source": [
        "### Extension attributes\n",
        "\n",
        "Add custom attributes to the Doc, Token and Span objects to store custom data.\n",
        "\n",
        "- Custom attributes are available via the `dot-underscore` property. This makes it clear that they were added by the user, and not built into spaCy, like token dot text.\n",
        "\n",
        "- Attributes need to be registered on the `global Doc, Token and Span classes` we can import from spacy dot tokens. To register a custom attribute on the Doc, Token or Span, we can use the `set extension` method.\n",
        "\n",
        "- The first argument is the attribute name. Keyword arguments let's us define how the value should be computed. In this case, it has a default value and can be overwritten."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t1OUJ_LzNxcd",
        "colab_type": "code",
        "outputId": "b773e5b0-61ea-4bd4-a84d-e25000ee61e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        }
      },
      "source": [
        "# Import global classes\n",
        "from spacy.tokens import Doc, Token, Span\n",
        "\n",
        "# Set extensions on the Doc, Token and Span\n",
        "Doc.set_extension('title', default=None)\n",
        "Token.set_extension('is_color', default=False)\n",
        "Span.set_extension('has_color', default=False)\n",
        "\n",
        "# Add custom metadata to documents, tokens and spans\n",
        "doc._.title = 'My document'\n",
        "token._.is_color = True\n",
        "span._.has_color = False"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-94b30bd1d400>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'My document'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_color\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mspan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_color\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Import global classes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'token' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wMAUlvZGOKd8",
        "colab_type": "text"
      },
      "source": [
        "**Extension attribute type**s\n",
        "1. Attribute extensions\n",
        "2. Property extensions\n",
        "3. Method extensions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lmMvyNZvOJ1X",
        "colab_type": "code",
        "outputId": "6c2f2f4d-04be-49ba-ee50-470cc74ad443",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        }
      },
      "source": [
        "from spacy.tokens import Token\n",
        "\n",
        "# Set extension on the Token with default value\n",
        "Token.set_extension('is_color', default=False)\n",
        "\n",
        "doc = nlp(\"The sky is blue.\")\n",
        "\n",
        "# Overwrite extension attribute value\n",
        "doc[3]._.is_color = True"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-4e8818c71993>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Set extension on the Token with default value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mToken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_extension\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'is_color'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The sky is blue.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mtoken.pyx\u001b[0m in \u001b[0;36mspacy.tokens.token.Token.set_extension\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: [E090] Extension 'is_color' already exists on Token. To overwrite the existing extension, set `force=True` on `Token.set_extension`."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ErbJfKkO4RP",
        "colab_type": "text"
      },
      "source": [
        "**Attribute extensions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EWPpUNZnOtbR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set extension on the Token with default value\n",
        "Token.set_extension('is_color', default=False, force=True)\n",
        "\n",
        "doc = nlp(\"The sky is blue.\")\n",
        "\n",
        "# Overwrite extension attribute value\n",
        "doc[3]._.is_color = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O30Fx5HiO81o",
        "colab_type": "text"
      },
      "source": [
        "**Property extensions**\n",
        "\n",
        "- Define a getter and an optional setter function\n",
        "- Getter only called when we retrieve the attribute value\n",
        "\n",
        "\n",
        "This lets us compute the value dynamically, and even take other custom attributes into account.\n",
        "\n",
        "Getter functions take one argument: the object, in this case, the token. In this example, the function returns whether the token text is in our list of colors.\n",
        "\n",
        "We can then provide the function via the getter keyword argument when we register the extension."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "snnNtxLlPWq8",
        "colab_type": "code",
        "outputId": "bfcceb64-0bcd-43a9-d05e-d7589293afd1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Define getter function\n",
        "def get_is_color(token):\n",
        "    colors = ['red', 'yellow', 'blue']\n",
        "    return token.text in colors\n",
        "\n",
        "# Set extension on the Token with getter\n",
        "Token.set_extension('is_color', getter=get_is_color, force=True)\n",
        "\n",
        "doc = nlp(\"The sky is blue.\")\n",
        "print(doc[3]._.is_color, '-', doc[3].text)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True - blue\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lNsOTaBKPiUe",
        "colab_type": "code",
        "outputId": "c6ee6fe3-abc9-4fa8-a247-2bfc59635baa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Span extensions should almost always use a getter\n",
        "\n",
        "# Define getter function\n",
        "def get_has_color(span):\n",
        "    colors = ['red', 'yellow', 'blue']\n",
        "    return any(token.text in colors for token in span)\n",
        "\n",
        "# Set extension on the Span with getter\n",
        "Span.set_extension('has_color', getter=get_has_color, force=True)\n",
        "\n",
        "doc = nlp(\"The sky is blue.\")\n",
        "print(doc[1:4]._.has_color, '-', doc[1:4].text)\n",
        "print(doc[0:2]._.has_color, '-', doc[0:2].text)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True - sky is blue\n",
            "False - The sky\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z_SXrcPDW0_z",
        "colab_type": "text"
      },
      "source": [
        "**Method extensions**\n",
        "- Assign a function that becomes available as an object method\n",
        "- Lets us pass arguments to the extension function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VNTmN3yfW0VX",
        "colab_type": "code",
        "outputId": "b5f42bd7-a513-4855-aa4c-9e5bc865f0a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "from spacy.tokens import Doc\n",
        "\n",
        "# Define method with arguments\n",
        "def has_token(doc, token_text):\n",
        "    in_doc = token_text in [token.text for token in doc]\n",
        "    return in_doc\n",
        "\n",
        "# Set extension on the Doc with method\n",
        "Doc.set_extension('has_token', method=has_token, force=True)\n",
        "\n",
        "doc = nlp(\"The sky is blue.\")\n",
        "print(doc._.has_token('blue'), '- blue')\n",
        "print(doc._.has_token('cloud'), '- cloud')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True - blue\n",
            "False - cloud\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IH_1_Dk7X8Oz",
        "colab_type": "text"
      },
      "source": [
        "In this example, the method function checks whether the doc contains a token with a given text. The first argument of the method is always the object itself – in this case, the Doc. It's passed in automatically when the method is called. All other function arguments will be arguments on the method extension. In this case, \"token text\".\n",
        "\n",
        "Here, the custom \"has token\" method returns True for the word \"blue\" and False for the word \"cloud\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IaWNJ-axX-bt",
        "colab_type": "code",
        "outputId": "8f8ded40-4438-41e0-b002-fdaa67429907",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from spacy.lang.en import English\n",
        "from spacy.tokens import Token\n",
        "\n",
        "nlp = English()\n",
        "\n",
        "# Register the Token extension attribute 'is_country' with the default value False\n",
        "Token.set_extension('is_country', default=False)\n",
        "\n",
        "# Process the text and set the is_country attribute to True for the token \"Spain\"\n",
        "doc = nlp(\"I live in Spain.\")\n",
        "doc[3]._.is_country = True\n",
        "\n",
        "# Print the token text and the is_country attribute for all tokens\n",
        "print([(token.text, token._.is_country) for token in doc])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('I', False), ('live', False), ('in', False), ('Spain', True), ('.', False)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E2ub-CHVZ7dq",
        "colab_type": "code",
        "outputId": "be28eac8-632c-4a52-9af7-7bc8003a8066",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "# Define the getter function that takes a token and returns its reversed text\n",
        "def get_reversed(token):\n",
        "    return token.text[::-1]\n",
        "\n",
        "\n",
        "# Register the Token property extension 'reversed' with the getter get_reversed\n",
        "Token.set_extension(\"reversed\", getter=get_reversed, force=True)\n",
        "\n",
        "# Process the text and print the reversed attribute for each token\n",
        "doc = nlp(\"All generalizations are false, including this one.\")\n",
        "for token in doc:\n",
        "    print(\"reversed:\", token._.reversed)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "reversed: llA\n",
            "reversed: snoitazilareneg\n",
            "reversed: era\n",
            "reversed: eslaf\n",
            "reversed: ,\n",
            "reversed: gnidulcni\n",
            "reversed: siht\n",
            "reversed: eno\n",
            "reversed: .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aV-rL4rzbN_5",
        "colab_type": "code",
        "outputId": "f37774d9-b989-4d88-ca60-ab778bf2da74",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Define the getter function\n",
        "def get_has_number(doc):\n",
        "    # Return if any of the tokens in the doc return True for token.like_num\n",
        "    return any(token.like_num for token in doc)\n",
        "\n",
        "\n",
        "# Register the Doc property extension 'has_number' with the getter get_has_number\n",
        "Doc.set_extension('has_number', getter=get_has_number)\n",
        "\n",
        "# Process the text and check the custom has_number attribute\n",
        "doc = nlp(\"The museum closed for five years in 2012.\")\n",
        "print(\"has_number:\", doc._.has_number)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "has_number: True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SnDa_CmLb89U",
        "colab_type": "code",
        "outputId": "45ba7e35-d132-4b94-c06f-ee24c579d9d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from spacy.tokens import Span\n",
        "# Define the method\n",
        "def to_html(span, tag):\n",
        "    # Wrap the span text in a HTML tag and return it\n",
        "    return \"<{tag}>{text}</{tag}>\".format(tag=tag, text=span.text)\n",
        "\n",
        "\n",
        "# Register the Span property extension 'to_html' with the method to_html\n",
        "Span.set_extension(\"to_html\", method=to_html)\n",
        "\n",
        "# Process the text and call the to_html method on the span with the tag name 'strong'\n",
        "doc = nlp(\"Hello world, this is a sentence.\")\n",
        "span = doc[0:2]\n",
        "print(span._.to_html(\"strong\"))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<strong>Hello world</strong>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-2z9tms0cOrj",
        "colab_type": "code",
        "outputId": "793b261a-b806-4dd8-fbc4-6531b7a5d1ec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Entities and Extension\n",
        "\n",
        "# we’ll combine custom extension attributes with the model’s predictions and create an attribute getter that returns a Wikipedia search URL \n",
        "# if the span is a person, organization, or location.\n",
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "\n",
        "def get_wikipedia_url(span):\n",
        "    # Get a Wikipedia URL if the span has one of the labels\n",
        "    if span.label_ in (\"PERSON\", \"ORG\", \"GPE\", \"LOCATION\"):\n",
        "        entity_text = span.text.replace(\" \", \"_\")\n",
        "        return \"https://en.wikipedia.org/w/index.php?search=\" + entity_text\n",
        "\n",
        "\n",
        "# Set the Span extension wikipedia_url using get getter get_wikipedia_url\n",
        "Span.set_extension(\"wikipedia_url\", getter=get_wikipedia_url)\n",
        "\n",
        "doc = nlp(\n",
        "    \"In over fifty years from his very first recordings right through to his \"\n",
        "    \"last album, David Bowie was at the vanguard of contemporary culture.\"\n",
        ")\n",
        "for ent in doc.ents:\n",
        "    # Print the text and Wikipedia URL of the entity\n",
        "    print(ent.text, ent._.wikipedia_url)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fifty years None\n",
            "David Bowie https://en.wikipedia.org/w/index.php?search=David_Bowie\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KklWMdE5gNH5",
        "colab_type": "text"
      },
      "source": [
        "**Components with extension**\n",
        "\n",
        "Extension attributes are especially powerful if they’re combined with custom pipeline components. \n",
        "\n",
        "write a pipeline component that finds country names and a custom extension attribute that returns a country’s capital, if available."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uLHOyXS7GmUM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "e1198782-e294-45f4-86ce-13e2fb8a95fa"
      },
      "source": [
        "from spacy.matcher import PhraseMatcher\n",
        "from spacy.tokens import Span\n",
        "\n",
        "COUNTRIES = [\n",
        "    \"Afghanistan\",\n",
        "    \"Åland Islands\",\n",
        "    \"Albania\",\n",
        "    \"Algeria\",\n",
        "    \"American Samoa\",\n",
        "    \"Andorra\",\n",
        "    \"Angola\",\n",
        "    \"Anguilla\",\n",
        "    \"Antarctica\",\n",
        "    \"Antigua and Barbuda\",\n",
        "    \"Argentina\",\n",
        "    \"Armenia\",\n",
        "    \"Aruba\",\n",
        "    \"Australia\",\n",
        "    \"Austria\",\n",
        "    \"Azerbaijan\",\n",
        "    \"Bahamas\",\n",
        "    \"Bahrain\",\n",
        "    \"Bangladesh\",\n",
        "    \"Barbados\",\n",
        "    \"Belarus\",\n",
        "    \"Belgium\",\n",
        "    \"Belize\",\n",
        "    \"Benin\",\n",
        "    \"Bermuda\",\n",
        "    \"Bhutan\",\n",
        "    \"Bolivia (Plurinational State of)\",\n",
        "    \"Bonaire, Sint Eustatius and Saba\",\n",
        "    \"Bosnia and Herzegovina\",\n",
        "    \"Botswana\",\n",
        "    \"Bouvet Island\",\n",
        "    \"Brazil\",\n",
        "    \"British Indian Ocean Territory\",\n",
        "    \"United States Minor Outlying Islands\",\n",
        "    \"Virgin Islands (British)\",\n",
        "    \"Virgin Islands (U.S.)\",\n",
        "    \"Brunei Darussalam\",\n",
        "    \"Bulgaria\",\n",
        "    \"Burkina Faso\",\n",
        "    \"Burundi\",\n",
        "    \"Cambodia\",\n",
        "    \"Cameroon\",\n",
        "    \"Canada\",\n",
        "    \"Cabo Verde\",\n",
        "    \"Cayman Islands\",\n",
        "    \"Central African Republic\",\n",
        "    \"Chad\",\n",
        "    \"Chile\",\n",
        "    \"China\",\n",
        "    \"Christmas Island\",\n",
        "    \"Cocos (Keeling) Islands\",\n",
        "    \"Colombia\",\n",
        "    \"Comoros\",\n",
        "    \"Congo\",\n",
        "    \"Congo (Democratic Republic of the)\",\n",
        "    \"Cook Islands\",\n",
        "    \"Costa Rica\",\n",
        "    \"Croatia\",\n",
        "    \"Cuba\",\n",
        "    \"Curaçao\",\n",
        "    \"Cyprus\",\n",
        "    \"Czech Republic\",\n",
        "    \"Denmark\",\n",
        "    \"Djibouti\",\n",
        "    \"Dominica\",\n",
        "    \"Dominican Republic\",\n",
        "    \"Ecuador\",\n",
        "    \"Egypt\",\n",
        "    \"El Salvador\",\n",
        "    \"Equatorial Guinea\",\n",
        "    \"Eritrea\",\n",
        "    \"Estonia\",\n",
        "    \"Ethiopia\",\n",
        "    \"Falkland Islands (Malvinas)\",\n",
        "    \"Faroe Islands\",\n",
        "    \"Fiji\",\n",
        "    \"Finland\",\n",
        "    \"France\",\n",
        "    \"French Guiana\",\n",
        "    \"French Polynesia\",\n",
        "    \"French Southern Territories\",\n",
        "    \"Gabon\",\n",
        "    \"Gambia\",\n",
        "    \"Georgia\",\n",
        "    \"Germany\",\n",
        "    \"Ghana\",\n",
        "    \"Gibraltar\",\n",
        "    \"Greece\",\n",
        "    \"Greenland\",\n",
        "    \"Grenada\",\n",
        "    \"Guadeloupe\",\n",
        "    \"Guam\",\n",
        "    \"Guatemala\",\n",
        "    \"Guernsey\",\n",
        "    \"Guinea\",\n",
        "    \"Guinea-Bissau\",\n",
        "    \"Guyana\",\n",
        "    \"Haiti\",\n",
        "    \"Heard Island and McDonald Islands\",\n",
        "    \"Holy See\",\n",
        "    \"Honduras\",\n",
        "    \"Hong Kong\",\n",
        "    \"Hungary\",\n",
        "    \"Iceland\",\n",
        "    \"India\",\n",
        "    \"Indonesia\",\n",
        "    \"Côte d'Ivoire\",\n",
        "    \"Iran (Islamic Republic of)\",\n",
        "    \"Iraq\",\n",
        "    \"Ireland\",\n",
        "    \"Isle of Man\",\n",
        "    \"Israel\",\n",
        "    \"Italy\",\n",
        "    \"Jamaica\",\n",
        "    \"Japan\",\n",
        "    \"Jersey\",\n",
        "    \"Jordan\",\n",
        "    \"Kazakhstan\",\n",
        "    \"Kenya\",\n",
        "    \"Kiribati\",\n",
        "    \"Kuwait\",\n",
        "    \"Kyrgyzstan\",\n",
        "    \"Lao People's Democratic Republic\",\n",
        "    \"Latvia\",\n",
        "    \"Lebanon\",\n",
        "    \"Lesotho\",\n",
        "    \"Liberia\",\n",
        "    \"Libya\",\n",
        "    \"Liechtenstein\",\n",
        "    \"Lithuania\",\n",
        "    \"Luxembourg\",\n",
        "    \"Macao\",\n",
        "    \"Macedonia (the former Yugoslav Republic of)\",\n",
        "    \"Madagascar\",\n",
        "    \"Malawi\",\n",
        "    \"Malaysia\",\n",
        "    \"Maldives\",\n",
        "    \"Mali\",\n",
        "    \"Malta\",\n",
        "    \"Marshall Islands\",\n",
        "    \"Martinique\",\n",
        "    \"Mauritania\",\n",
        "    \"Mauritius\",\n",
        "    \"Mayotte\",\n",
        "    \"Mexico\",\n",
        "    \"Micronesia (Federated States of)\",\n",
        "    \"Moldova (Republic of)\",\n",
        "    \"Monaco\",\n",
        "    \"Mongolia\",\n",
        "    \"Montenegro\",\n",
        "    \"Montserrat\",\n",
        "    \"Morocco\",\n",
        "    \"Mozambique\",\n",
        "    \"Myanmar\",\n",
        "    \"Namibia\",\n",
        "    \"Nauru\",\n",
        "    \"Nepal\",\n",
        "    \"Netherlands\",\n",
        "    \"New Caledonia\",\n",
        "    \"New Zealand\",\n",
        "    \"Nicaragua\",\n",
        "    \"Niger\",\n",
        "    \"Nigeria\",\n",
        "    \"Niue\",\n",
        "    \"Norfolk Island\",\n",
        "    \"Korea (Democratic People's Republic of)\",\n",
        "    \"Northern Mariana Islands\",\n",
        "    \"Norway\",\n",
        "    \"Oman\",\n",
        "    \"Pakistan\",\n",
        "    \"Palau\",\n",
        "    \"Palestine, State of\",\n",
        "    \"Panama\",\n",
        "    \"Papua New Guinea\",\n",
        "    \"Paraguay\",\n",
        "    \"Peru\",\n",
        "    \"Philippines\",\n",
        "    \"Pitcairn\",\n",
        "    \"Poland\",\n",
        "    \"Portugal\",\n",
        "    \"Puerto Rico\",\n",
        "    \"Qatar\",\n",
        "    \"Republic of Kosovo\",\n",
        "    \"Réunion\",\n",
        "    \"Romania\",\n",
        "    \"Russian Federation\",\n",
        "    \"Rwanda\",\n",
        "    \"Saint Barthélemy\",\n",
        "    \"Saint Helena, Ascension and Tristan da Cunha\",\n",
        "    \"Saint Kitts and Nevis\",\n",
        "    \"Saint Lucia\",\n",
        "    \"Saint Martin (French part)\",\n",
        "    \"Saint Pierre and Miquelon\",\n",
        "    \"Saint Vincent and the Grenadines\",\n",
        "    \"Samoa\",\n",
        "    \"San Marino\",\n",
        "    \"Sao Tome and Principe\",\n",
        "    \"Saudi Arabia\",\n",
        "    \"Senegal\",\n",
        "    \"Serbia\",\n",
        "    \"Seychelles\",\n",
        "    \"Sierra Leone\",\n",
        "    \"Singapore\",\n",
        "    \"Sint Maarten (Dutch part)\",\n",
        "    \"Slovakia\",\n",
        "    \"Slovenia\",\n",
        "    \"Solomon Islands\",\n",
        "    \"Somalia\",\n",
        "    \"South Africa\",\n",
        "    \"South Georgia and the South Sandwich Islands\",\n",
        "    \"Korea (Republic of)\",\n",
        "    \"South Sudan\",\n",
        "    \"Spain\",\n",
        "    \"Sri Lanka\",\n",
        "    \"Sudan\",\n",
        "    \"Suriname\",\n",
        "    \"Svalbard and Jan Mayen\",\n",
        "    \"Swaziland\",\n",
        "    \"Sweden\",\n",
        "    \"Switzerland\",\n",
        "    \"Syrian Arab Republic\",\n",
        "    \"Taiwan\",\n",
        "    \"Tajikistan\",\n",
        "    \"Tanzania, United Republic of\",\n",
        "    \"Thailand\",\n",
        "    \"Timor-Leste\",\n",
        "    \"Togo\",\n",
        "    \"Tokelau\",\n",
        "    \"Tonga\",\n",
        "    \"Trinidad and Tobago\",\n",
        "    \"Tunisia\",\n",
        "    \"Turkey\",\n",
        "    \"Turkmenistan\",\n",
        "    \"Turks and Caicos Islands\",\n",
        "    \"Tuvalu\",\n",
        "    \"Uganda\",\n",
        "    \"Ukraine\",\n",
        "    \"United Arab Emirates\",\n",
        "    \"United Kingdom of Great Britain and Northern Ireland\",\n",
        "    \"United States of America\",\n",
        "    \"Uruguay\",\n",
        "    \"Uzbekistan\",\n",
        "    \"Vanuatu\",\n",
        "    \"Venezuela (Bolivarian Republic of)\",\n",
        "    \"Viet Nam\",\n",
        "    \"Wallis and Futuna\",\n",
        "    \"Western Sahara\",\n",
        "    \"Yemen\",\n",
        "    \"Zambia\",\n",
        "    \"Zimbabwe\"\n",
        "]\n",
        "\n",
        "CAPITALS = {\n",
        "  \"Afghanistan\":\"Kabul\",\n",
        "  \"\\u00c5land Islands\":\"Mariehamn\",\n",
        "  \"Albania\":\"Tirana\",\n",
        "  \"Algeria\":\"Algiers\",\n",
        "  \"American Samoa\":\"Pago Pago\",\n",
        "  \"Andorra\":\"Andorra la Vella\",\n",
        "  \"Angola\":\"Luanda\",\n",
        "  \"Anguilla\":\"The Valley\",\n",
        "  \"Antarctica\":\"\",\n",
        "  \"Antigua and Barbuda\":\"Saint John's\",\n",
        "  \"Argentina\":\"Buenos Aires\",\n",
        "  \"Armenia\":\"Yerevan\",\n",
        "  \"Aruba\":\"Oranjestad\",\n",
        "  \"Australia\":\"Canberra\",\n",
        "  \"Austria\":\"Vienna\",\n",
        "  \"Azerbaijan\":\"Baku\",\n",
        "  \"Bahamas\":\"Nassau\",\n",
        "  \"Bahrain\":\"Manama\",\n",
        "  \"Bangladesh\":\"Dhaka\",\n",
        "  \"Barbados\":\"Bridgetown\",\n",
        "  \"Belarus\":\"Minsk\",\n",
        "  \"Belgium\":\"Brussels\",\n",
        "  \"Belize\":\"Belmopan\",\n",
        "  \"Benin\":\"Porto-Novo\",\n",
        "  \"Bermuda\":\"Hamilton\",\n",
        "  \"Bhutan\":\"Thimphu\",\n",
        "  \"Bolivia (Plurinational State of)\":\"Sucre\",\n",
        "  \"Bonaire, Sint Eustatius and Saba\":\"Kralendijk\",\n",
        "  \"Bosnia and Herzegovina\":\"Sarajevo\",\n",
        "  \"Botswana\":\"Gaborone\",\n",
        "  \"Bouvet Island\":\"\",\n",
        "  \"Brazil\":\"Bras\\u00edlia\",\n",
        "  \"British Indian Ocean Territory\":\"Diego Garcia\",\n",
        "  \"United States Minor Outlying Islands\":\"\",\n",
        "  \"Virgin Islands (British)\":\"Road Town\",\n",
        "  \"Virgin Islands (U.S.)\":\"Charlotte Amalie\",\n",
        "  \"Brunei Darussalam\":\"Bandar Seri Begawan\",\n",
        "  \"Bulgaria\":\"Sofia\",\n",
        "  \"Burkina Faso\":\"Ouagadougou\",\n",
        "  \"Burundi\":\"Bujumbura\",\n",
        "  \"Cambodia\":\"Phnom Penh\",\n",
        "  \"Cameroon\":\"Yaound\\u00e9\",\n",
        "  \"Canada\":\"Ottawa\",\n",
        "  \"Cabo Verde\":\"Praia\",\n",
        "  \"Cayman Islands\":\"George Town\",\n",
        "  \"Central African Republic\":\"Bangui\",\n",
        "  \"Chad\":\"N'Djamena\",\n",
        "  \"Chile\":\"Santiago\",\n",
        "  \"China\":\"Beijing\",\n",
        "  \"Christmas Island\":\"Flying Fish Cove\",\n",
        "  \"Cocos (Keeling) Islands\":\"West Island\",\n",
        "  \"Colombia\":\"Bogot\\u00e1\",\n",
        "  \"Comoros\":\"Moroni\",\n",
        "  \"Congo\":\"Brazzaville\",\n",
        "  \"Congo (Democratic Republic of the)\":\"Kinshasa\",\n",
        "  \"Cook Islands\":\"Avarua\",\n",
        "  \"Costa Rica\":\"San Jos\\u00e9\",\n",
        "  \"Croatia\":\"Zagreb\",\n",
        "  \"Cuba\":\"Havana\",\n",
        "  \"Cura\\u00e7ao\":\"Willemstad\",\n",
        "  \"Cyprus\":\"Nicosia\",\n",
        "  \"Czech Republic\":\"Prague\",\n",
        "  \"Denmark\":\"Copenhagen\",\n",
        "  \"Djibouti\":\"Djibouti\",\n",
        "  \"Dominica\":\"Roseau\",\n",
        "  \"Dominican Republic\":\"Santo Domingo\",\n",
        "  \"Ecuador\":\"Quito\",\n",
        "  \"Egypt\":\"Cairo\",\n",
        "  \"El Salvador\":\"San Salvador\",\n",
        "  \"Equatorial Guinea\":\"Malabo\",\n",
        "  \"Eritrea\":\"Asmara\",\n",
        "  \"Estonia\":\"Tallinn\",\n",
        "  \"Ethiopia\":\"Addis Ababa\",\n",
        "  \"Falkland Islands (Malvinas)\":\"Stanley\",\n",
        "  \"Faroe Islands\":\"T\\u00f3rshavn\",\n",
        "  \"Fiji\":\"Suva\",\n",
        "  \"Finland\":\"Helsinki\",\n",
        "  \"France\":\"Paris\",\n",
        "  \"French Guiana\":\"Cayenne\",\n",
        "  \"French Polynesia\":\"Papeet\\u0113\",\n",
        "  \"French Southern Territories\":\"Port-aux-Fran\\u00e7ais\",\n",
        "  \"Gabon\":\"Libreville\",\n",
        "  \"Gambia\":\"Banjul\",\n",
        "  \"Georgia\":\"Tbilisi\",\n",
        "  \"Germany\":\"Berlin\",\n",
        "  \"Ghana\":\"Accra\",\n",
        "  \"Gibraltar\":\"Gibraltar\",\n",
        "  \"Greece\":\"Athens\",\n",
        "  \"Greenland\":\"Nuuk\",\n",
        "  \"Grenada\":\"St. George's\",\n",
        "  \"Guadeloupe\":\"Basse-Terre\",\n",
        "  \"Guam\":\"Hag\\u00e5t\\u00f1a\",\n",
        "  \"Guatemala\":\"Guatemala City\",\n",
        "  \"Guernsey\":\"St. Peter Port\",\n",
        "  \"Guinea\":\"Conakry\",\n",
        "  \"Guinea-Bissau\":\"Bissau\",\n",
        "  \"Guyana\":\"Georgetown\",\n",
        "  \"Haiti\":\"Port-au-Prince\",\n",
        "  \"Heard Island and McDonald Islands\":\"\",\n",
        "  \"Holy See\":\"Rome\",\n",
        "  \"Honduras\":\"Tegucigalpa\",\n",
        "  \"Hong Kong\":\"City of Victoria\",\n",
        "  \"Hungary\":\"Budapest\",\n",
        "  \"Iceland\":\"Reykjav\\u00edk\",\n",
        "  \"India\":\"New Delhi\",\n",
        "  \"Indonesia\":\"Jakarta\",\n",
        "  \"C\\u00f4te d'Ivoire\":\"Yamoussoukro\",\n",
        "  \"Iran (Islamic Republic of)\":\"Tehran\",\n",
        "  \"Iraq\":\"Baghdad\",\n",
        "  \"Ireland\":\"Dublin\",\n",
        "  \"Isle of Man\":\"Douglas\",\n",
        "  \"Israel\":\"Jerusalem\",\n",
        "  \"Italy\":\"Rome\",\n",
        "  \"Jamaica\":\"Kingston\",\n",
        "  \"Japan\":\"Tokyo\",\n",
        "  \"Jersey\":\"Saint Helier\",\n",
        "  \"Jordan\":\"Amman\",\n",
        "  \"Kazakhstan\":\"Astana\",\n",
        "  \"Kenya\":\"Nairobi\",\n",
        "  \"Kiribati\":\"South Tarawa\",\n",
        "  \"Kuwait\":\"Kuwait City\",\n",
        "  \"Kyrgyzstan\":\"Bishkek\",\n",
        "  \"Lao People's Democratic Republic\":\"Vientiane\",\n",
        "  \"Latvia\":\"Riga\",\n",
        "  \"Lebanon\":\"Beirut\",\n",
        "  \"Lesotho\":\"Maseru\",\n",
        "  \"Liberia\":\"Monrovia\",\n",
        "  \"Libya\":\"Tripoli\",\n",
        "  \"Liechtenstein\":\"Vaduz\",\n",
        "  \"Lithuania\":\"Vilnius\",\n",
        "  \"Luxembourg\":\"Luxembourg\",\n",
        "  \"Macao\":\"\",\n",
        "  \"Macedonia (the former Yugoslav Republic of)\":\"Skopje\",\n",
        "  \"Madagascar\":\"Antananarivo\",\n",
        "  \"Malawi\":\"Lilongwe\",\n",
        "  \"Malaysia\":\"Kuala Lumpur\",\n",
        "  \"Maldives\":\"Mal\\u00e9\",\n",
        "  \"Mali\":\"Bamako\",\n",
        "  \"Malta\":\"Valletta\",\n",
        "  \"Marshall Islands\":\"Majuro\",\n",
        "  \"Martinique\":\"Fort-de-France\",\n",
        "  \"Mauritania\":\"Nouakchott\",\n",
        "  \"Mauritius\":\"Port Louis\",\n",
        "  \"Mayotte\":\"Mamoudzou\",\n",
        "  \"Mexico\":\"Mexico City\",\n",
        "  \"Micronesia (Federated States of)\":\"Palikir\",\n",
        "  \"Moldova (Republic of)\":\"Chi\\u0219in\\u0103u\",\n",
        "  \"Monaco\":\"Monaco\",\n",
        "  \"Mongolia\":\"Ulan Bator\",\n",
        "  \"Montenegro\":\"Podgorica\",\n",
        "  \"Montserrat\":\"Plymouth\",\n",
        "  \"Morocco\":\"Rabat\",\n",
        "  \"Mozambique\":\"Maputo\",\n",
        "  \"Myanmar\":\"Naypyidaw\",\n",
        "  \"Namibia\":\"Windhoek\",\n",
        "  \"Nauru\":\"Yaren\",\n",
        "  \"Nepal\":\"Kathmandu\",\n",
        "  \"Netherlands\":\"Amsterdam\",\n",
        "  \"New Caledonia\":\"Noum\\u00e9a\",\n",
        "  \"New Zealand\":\"Wellington\",\n",
        "  \"Nicaragua\":\"Managua\",\n",
        "  \"Niger\":\"Niamey\",\n",
        "  \"Nigeria\":\"Abuja\",\n",
        "  \"Niue\":\"Alofi\",\n",
        "  \"Norfolk Island\":\"Kingston\",\n",
        "  \"Korea (Democratic People's Republic of)\":\"Pyongyang\",\n",
        "  \"Northern Mariana Islands\":\"Saipan\",\n",
        "  \"Norway\":\"Oslo\",\n",
        "  \"Oman\":\"Muscat\",\n",
        "  \"Pakistan\":\"Islamabad\",\n",
        "  \"Palau\":\"Ngerulmud\",\n",
        "  \"Palestine, State of\":\"Ramallah\",\n",
        "  \"Panama\":\"Panama City\",\n",
        "  \"Papua New Guinea\":\"Port Moresby\",\n",
        "  \"Paraguay\":\"Asunci\\u00f3n\",\n",
        "  \"Peru\":\"Lima\",\n",
        "  \"Philippines\":\"Manila\",\n",
        "  \"Pitcairn\":\"Adamstown\",\n",
        "  \"Poland\":\"Warsaw\",\n",
        "  \"Portugal\":\"Lisbon\",\n",
        "  \"Puerto Rico\":\"San Juan\",\n",
        "  \"Qatar\":\"Doha\",\n",
        "  \"Republic of Kosovo\":\"Pristina\",\n",
        "  \"R\\u00e9union\":\"Saint-Denis\",\n",
        "  \"Romania\":\"Bucharest\",\n",
        "  \"Russian Federation\":\"Moscow\",\n",
        "  \"Rwanda\":\"Kigali\",\n",
        "  \"Saint Barth\\u00e9lemy\":\"Gustavia\",\n",
        "  \"Saint Helena, Ascension and Tristan da Cunha\":\"Jamestown\",\n",
        "  \"Saint Kitts and Nevis\":\"Basseterre\",\n",
        "  \"Saint Lucia\":\"Castries\",\n",
        "  \"Saint Martin (French part)\":\"Marigot\",\n",
        "  \"Saint Pierre and Miquelon\":\"Saint-Pierre\",\n",
        "  \"Saint Vincent and the Grenadines\":\"Kingstown\",\n",
        "  \"Samoa\":\"Apia\",\n",
        "  \"San Marino\":\"City of San Marino\",\n",
        "  \"Sao Tome and Principe\":\"S\\u00e3o Tom\\u00e9\",\n",
        "  \"Saudi Arabia\":\"Riyadh\",\n",
        "  \"Senegal\":\"Dakar\",\n",
        "  \"Serbia\":\"Belgrade\",\n",
        "  \"Seychelles\":\"Victoria\",\n",
        "  \"Sierra Leone\":\"Freetown\",\n",
        "  \"Singapore\":\"Singapore\",\n",
        "  \"Sint Maarten (Dutch part)\":\"Philipsburg\",\n",
        "  \"Slovakia\":\"Bratislava\",\n",
        "  \"Slovenia\":\"Ljubljana\",\n",
        "  \"Solomon Islands\":\"Honiara\",\n",
        "  \"Somalia\":\"Mogadishu\",\n",
        "  \"South Africa\":\"Pretoria\",\n",
        "  \"South Georgia and the South Sandwich Islands\":\"King Edward Point\",\n",
        "  \"Korea (Republic of)\":\"Seoul\",\n",
        "  \"South Sudan\":\"Juba\",\n",
        "  \"Spain\":\"Madrid\",\n",
        "  \"Sri Lanka\":\"Colombo\",\n",
        "  \"Sudan\":\"Khartoum\",\n",
        "  \"Suriname\":\"Paramaribo\",\n",
        "  \"Svalbard and Jan Mayen\":\"Longyearbyen\",\n",
        "  \"Swaziland\":\"Lobamba\",\n",
        "  \"Sweden\":\"Stockholm\",\n",
        "  \"Switzerland\":\"Bern\",\n",
        "  \"Syrian Arab Republic\":\"Damascus\",\n",
        "  \"Taiwan\":\"Taipei\",\n",
        "  \"Tajikistan\":\"Dushanbe\",\n",
        "  \"Tanzania, United Republic of\":\"Dodoma\",\n",
        "  \"Thailand\":\"Bangkok\",\n",
        "  \"Timor-Leste\":\"Dili\",\n",
        "  \"Togo\":\"Lom\\u00e9\",\n",
        "  \"Tokelau\":\"Fakaofo\",\n",
        "  \"Tonga\":\"Nuku'alofa\",\n",
        "  \"Trinidad and Tobago\":\"Port of Spain\",\n",
        "  \"Tunisia\":\"Tunis\",\n",
        "  \"Turkey\":\"Ankara\",\n",
        "  \"Turkmenistan\":\"Ashgabat\",\n",
        "  \"Turks and Caicos Islands\":\"Cockburn Town\",\n",
        "  \"Tuvalu\":\"Funafuti\",\n",
        "  \"Uganda\":\"Kampala\",\n",
        "  \"Ukraine\":\"Kiev\",\n",
        "  \"United Arab Emirates\":\"Abu Dhabi\",\n",
        "  \"United Kingdom of Great Britain and Northern Ireland\":\"London\",\n",
        "  \"United States of America\":\"Washington, D.C.\",\n",
        "  \"Uruguay\":\"Montevideo\",\n",
        "  \"Uzbekistan\":\"Tashkent\",\n",
        "  \"Vanuatu\":\"Port Vila\",\n",
        "  \"Venezuela (Bolivarian Republic of)\":\"Caracas\",\n",
        "  \"Viet Nam\":\"Hanoi\",\n",
        "  \"Wallis and Futuna\":\"Mata-Utu\",\n",
        "  \"Western Sahara\":\"El Aai\\u00fan\",\n",
        "  \"Yemen\":\"Sana'a\",\n",
        "  \"Zambia\":\"Lusaka\",\n",
        "  \"Zimbabwe\":\"Harare\"\n",
        "}\n",
        "\n",
        "nlp = English()\n",
        "matcher = PhraseMatcher(nlp.vocab)\n",
        "matcher.add(\"COUNTRY\", None, *list(nlp.pipe(COUNTRIES)))\n",
        " \n",
        "def countries_component(doc):\n",
        "  # Create an entity Span with the label 'GPE' for all matches\n",
        "  matches = matcher(doc)\n",
        "  doc.ents = [Span(doc, start, end, label='GPE') for match_id, start, end in matches]\n",
        "  return doc\n",
        " \n",
        "# Add the component to the pipeline\n",
        "nlp.add_pipe(countries_component)\n",
        "print(nlp.pipe_names)\n",
        " \n",
        "# Getter that looks up the span text in the dictionary of country capitals\n",
        "get_capital = lambda span: CAPITALS.get(span.text)\n",
        " \n",
        "# Register the Span extension attribute 'capital' with the getter get_capital\n",
        "Span.set_extension('capital', getter= get_capital)\n",
        " \n",
        "# Process the text and print the entity text, label and capital attributes\n",
        "doc = nlp(\"Czech Republic may help Slovakia protect its airspace\")\n",
        "print([(ent.text, ent.label_, ent._.capital) for ent in doc.ents])"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['countries_component']\n",
            "[('Czech Republic', 'GPE', 'Prague'), ('Slovakia', 'GPE', 'Bratislava')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HXGzfaaOg-Kn",
        "colab_type": "text"
      },
      "source": [
        "### Scaling and performance\n",
        "\n",
        "**Processing large volumes of text**\n",
        "- Use `nlp.pipe` method\n",
        "- Processes texts as a stream, yields Doc objects\n",
        "- Much faster than calling nlp on each text\n",
        "\n",
        "**BAD:**\n",
        "\n",
        "`docs = [nlp(text) for text in LOTS_OF_TEXTS]`\n",
        "\n",
        "**GOOD:**\n",
        "\n",
        "`docs = list(nlp.pipe(LOTS_OF_TEXTS))`\n",
        "\n",
        "`nlp.pipe` is a generator that yields Doc objects"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x1cM0t5fhyHC",
        "colab_type": "text"
      },
      "source": [
        "**Passing in context**\n",
        "- Setting `as_tuples=True` on `nlp.pipe` lets you pass in `(text, context)` tuples\n",
        "- Yields `(doc, context)` tuples\n",
        "- Useful for associating metadata with the doc"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F-YmUSsxgpkx",
        "colab_type": "code",
        "outputId": "09d8906a-3f5c-45c0-ba17-fa2015a5c16b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "data = [\n",
        "    ('This is a text', {'id': 1, 'page_number': 15}),\n",
        "    ('And another text', {'id': 2, 'page_number': 16}),\n",
        "]\n",
        "\n",
        "# This is useful for passing in additional metadata, like an ID associated with the text, or a page number.\n",
        "for doc, context in nlp.pipe(data, as_tuples=True):\n",
        "    print(doc.text, context['page_number'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "This is a text 15\n",
            "And another text 16\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nPRHyykRincx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#  we're registering two extensions, \"id\" and \"page number\", which default to None.\n",
        "# After processing the text and passing through the context, we can overwrite the doc extensions with our context metadata.\n",
        "from spacy.tokens import Doc\n",
        "\n",
        "Doc.set_extension('id', default=None)\n",
        "Doc.set_extension('page_number', default=None)\n",
        "\n",
        "data = [\n",
        "    ('This is a text', {'id': 1, 'page_number': 15}),\n",
        "    ('And another text', {'id': 2, 'page_number': 16}),\n",
        "]\n",
        "\n",
        "for doc, context in nlp.pipe(data, as_tuples=True):\n",
        "    doc._.id = context['id']\n",
        "    doc._.page_number = context['page_number']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4SMc0kHDi-si",
        "colab_type": "text"
      },
      "source": [
        "**Note:** Sometimes you already have a model loaded to do other processing, but you only need the tokenizer for one particular text.\n",
        "\n",
        "Running the whole pipeline is unnecessarily slow, because you'll be getting a bunch of predictions from the model that you don't need.\n",
        "\n",
        "**BAD:**\n",
        "\n",
        "`doc = nlp(\"Hello world\")`\n",
        "\n",
        "**GOOD:**\n",
        "\n",
        "`doc = nlp.make_doc(\"Hello world!\")`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XxkO73IHi1tf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# If we only need a tokenized Doc object, we can use the nlp.make_doc method instead, which takes a text and returns a Doc.\n",
        "# nlp.make_doc turns the text into a Doc before the pipeline components are called"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-6GzPPE3jtqD",
        "colab_type": "text"
      },
      "source": [
        "**Disabling pipeline components**\n",
        "\n",
        "- Use `nlp.disable_pipes` to temporarily disable one or more pipes\n",
        "- Restores them after the `with` block\n",
        "- Only runs the remaining components"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Co_mUkrkBWw",
        "colab_type": "code",
        "outputId": "28c572ee-dc05-4715-fbb4-f4357792c46f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "text = (\n",
        "    \"Chick-fil-A is an American fast food restaurant chain headquartered in \"\n",
        "    \"the city of College Park, Georgia, specializing in chicken sandwiches.\"\n",
        ")\n",
        "\n",
        "# Disable the tagger and parser\n",
        "with nlp.disable_pipes('tagger','parser'):\n",
        "    # Process the text\n",
        "    doc = nlp(text)\n",
        "    # Print the entities in the doc\n",
        "    print(doc.ents)\n",
        "\n",
        "print(doc.ents)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(American, College Park, Georgia)\n",
            "(American, College Park, Georgia)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g67ocDnlF3nF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "8a3c5e21-f6ee-46b2-f46a-3066fba8d3d3"
      },
      "source": [
        "TEXTS = [\n",
        "    \"McDonalds is my favorite restaurant.\",\n",
        "    \"Here I thought @McDonalds only had precooked burgers but it seems they only have not cooked ones?? I have no time to get sick..\",\n",
        "    \"People really still eat McDonalds :(\",\n",
        "    \"The McDonalds in Spain has chicken wings. My heart is so happy \",\n",
        "    \"@McDonalds Please bring back the most delicious fast food sandwich of all times!!....The Arch Deluxe :P\",\n",
        "    \"please hurry and open. I WANT A #McRib SANDWICH SO BAD! :D\",\n",
        "    \"This morning i made a terrible decision by gettin mcdonalds and now my stomach is payin for it\"\n",
        "]\n",
        "\n",
        "import json\n",
        "import spacy\n",
        " \n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        " \n",
        "# with open(\"exercises/tweets.json\") as f:\n",
        "#   TEXTS = json.loads(f.read())\n",
        " \n",
        "# Process the texts and print the adjectives\n",
        "for text in TEXTS:\n",
        "  doc = nlp(text)\n",
        "  print([token.text for token in doc if token.pos_ == \"ADJ\"])"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['favorite']\n",
            "['sick']\n",
            "[]\n",
            "['happy']\n",
            "['delicious', 'fast']\n",
            "[]\n",
            "['terrible']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7aKtmc0BGDfh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "040bc9ba-2e18-47ee-e16c-f6b03fccee70"
      },
      "source": [
        "# Using nlp.pipe\n",
        "\n",
        "# Process the texts and print the adjectives\n",
        "for doc in nlp.pipe(TEXTS):\n",
        "    print([token.text for token in doc if token.pos_ == \"ADJ\"])"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['favorite']\n",
            "['sick']\n",
            "[]\n",
            "['happy']\n",
            "['delicious', 'fast']\n",
            "[]\n",
            "['terrible']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F8VaWixAof8Y",
        "colab_type": "text"
      },
      "source": [
        "Custom attributes to add author and book meta information to quotes.\n",
        "\n",
        "A list of `[text, context]` examples is available as the variable DATA. \n",
        "\n",
        "The texts are quotes from famous books, and the contexts dictionaries with the keys 'author' and 'book'."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HJslZIQzH3k7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DATA = [\n",
        "    [\n",
        "        \"One morning, when Gregor Samsa woke from troubled dreams, he found himself transformed in his bed into a horrible vermin.\",\n",
        "        { \"author\": \"Franz Kafka\", \"book\": \"Metamorphosis\" }\n",
        "    ],\n",
        "    [\n",
        "        \"I know not all that may be coming, but be it what it will, I'll go to it laughing.\",\n",
        "        { \"author\": \"Herman Melville\", \"book\": \"Moby-Dick or, The Whale\" }\n",
        "    ],\n",
        "    [\n",
        "        \"It was the best of times, it was the worst of times.\",\n",
        "        { \"author\": \"Charles Dickens\", \"book\": \"A Tale of Two Cities\" }\n",
        "    ],\n",
        "    [\n",
        "        \"The only people for me are the mad ones, the ones who are mad to live, mad to talk, mad to be saved, desirous of everything at the same time, the ones who never yawn or say a commonplace thing, but burn, burn, burn like fabulous yellow roman candles exploding like spiders across the stars.\",\n",
        "        { \"author\": \"Jack Kerouac\", \"book\": \"On the Road\" }\n",
        "    ],\n",
        "    [\n",
        "        \"It was a bright cold day in April, and the clocks were striking thirteen.\",\n",
        "        { \"author\": \"George Orwell\", \"book\": \"1984\" }\n",
        "    ],\n",
        "    [\n",
        "        \"Nowadays people know the price of everything and the value of nothing.\",\n",
        "        { \"author\": \"Oscar Wilde\", \"book\": \"The Picture Of Dorian Gray\" }\n",
        "    ]\n",
        "]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QTXcmDFPH_sJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        },
        "outputId": "891af26a-feac-4e60-8d44-1c96fe654027"
      },
      "source": [
        "nlp = English()\n",
        " \n",
        "# Register the Doc extension 'author' (default None)\n",
        "Doc.set_extension('author', default=None, force=True)\n",
        " \n",
        "# Register the Doc extension 'book' (default None)\n",
        "Doc.set_extension('book', default=None, force=True)\n",
        " \n",
        "for doc, context in nlp.pipe(DATA, as_tuples=True):\n",
        "  # Set the doc._.book and doc._.author attributes from the context\n",
        "  doc._.book = context['book']\n",
        "  doc._.author = context['author']\n",
        " \n",
        "  # Print the text and custom attribute data\n",
        "  print(doc.text, \"\\n\", \"— '{}' by {}\".format(doc._.book, doc._.author), \"\\n\")"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "One morning, when Gregor Samsa woke from troubled dreams, he found himself transformed in his bed into a horrible vermin. \n",
            " — 'Metamorphosis' by Franz Kafka \n",
            "\n",
            "I know not all that may be coming, but be it what it will, I'll go to it laughing. \n",
            " — 'Moby-Dick or, The Whale' by Herman Melville \n",
            "\n",
            "It was the best of times, it was the worst of times. \n",
            " — 'A Tale of Two Cities' by Charles Dickens \n",
            "\n",
            "The only people for me are the mad ones, the ones who are mad to live, mad to talk, mad to be saved, desirous of everything at the same time, the ones who never yawn or say a commonplace thing, but burn, burn, burn like fabulous yellow roman candles exploding like spiders across the stars. \n",
            " — 'On the Road' by Jack Kerouac \n",
            "\n",
            "It was a bright cold day in April, and the clocks were striking thirteen. \n",
            " — '1984' by George Orwell \n",
            "\n",
            "Nowadays people know the price of everything and the value of nothing. \n",
            " — 'The Picture Of Dorian Gray' by Oscar Wilde \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pogoV0DQp3cq",
        "colab_type": "text"
      },
      "source": [
        "# Chapter 4: Training a neural network model\n",
        "\n",
        "How to update spaCy's statistical models to customize them for our use case – for example, to predict a new entity type in online comments. we'll write our own training loop from scratch, and understand the basics of how training works, along with tips and tricks that can make our custom NLP projects more successful."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6NvrRqtUqJ3d",
        "colab_type": "text"
      },
      "source": [
        "**Why updating the model?**\n",
        "- Better results on our specific domain\n",
        "- Learn classification schemes specifically for our problem\n",
        "- Essential for text classification\n",
        "- Very useful for named entity recognition\n",
        "- Less critical for part-of-speech tagging and dependency parsing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IqAkdL4hqfWJ",
        "colab_type": "text"
      },
      "source": [
        "**How training works**\n",
        "1. Initialize the model weights randomly with nlp.begin_training\n",
        "2. Predict a few examples with the current weights by calling nlp.update\n",
        "3. Compare prediction with true labels\n",
        "4. Calculate how to change weights to improve predictions\n",
        "5. Update weights slightly\n",
        "6. Go back to 2.\n",
        "\n",
        "![alt text](https://course.spacy.io/training.png)\n",
        "\n",
        "**Training data:** Examples and their annotations.\n",
        "\n",
        "**Text:** The input text the model should predict a label for.\n",
        "\n",
        "**Label:** The label the model should predict.\n",
        "\n",
        "**Gradient:**How to change the weights."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w15ZZ9pA5_iM",
        "colab_type": "text"
      },
      "source": [
        "### Training\n",
        "The training data tells the model what we want it to predict. This could be texts and named entities we want to recognize, or tokens and their correct part-of-speech tags.\n",
        "\n",
        "To update an existing model, we can start with a few hundred to a few thousand examples.\n",
        "\n",
        "To train a new category we may need up to a million.\n",
        "\n",
        "spaCy's pre-trained English models for instance were trained on 2 million words labelled with part-of-speech tags, dependencies and named entities.\n",
        "\n",
        "Training data is usually created by humans who assign labels to texts.\n",
        "\n",
        "This is a lot of work, but can be semi-automated – for example, using spaCy's Matcher."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UvxsPZjZ6aam",
        "colab_type": "text"
      },
      "source": [
        "**spaCy’s components are supervised models for text annotations**\n",
        "\n",
        "spaCy’s rule-based Matcher is a great way to quickly create training data for named entity models. A list of sentences is available as the variable TEXTS. You can print it the IPython shell to inspect it. We want to find all mentions of different iPhone models, so we can create training data to teach a model to recognize them as 'GADGET'."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sh5mwygdq4_K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "from spacy.matcher import Matcher\n",
        "from spacy.lang.en import English\n",
        "\n",
        "# with open(\"exercises/iphone.json\") as f:\n",
        "#     TEXTS = json.loads(f.read())\n",
        "\n",
        "TEXTS = [\n",
        "  \"How to preorder the iPhone X\",\n",
        "  \"iPhone X is coming\",\n",
        "  \"Should I pay $1,000 for the iPhone X?\",\n",
        "  \"The iPhone 8 reviews are here\",\n",
        "  \"Your iPhone goes up to 11 today\",\n",
        "  \"I need a new phone! Any tips?\"\n",
        "]\n",
        "\n",
        "nlp = English()\n",
        "matcher = Matcher(nlp.vocab)\n",
        "\n",
        "# Two tokens whose lowercase forms match 'iphone' and 'x'\n",
        "pattern1 = [{\"LOWER\": \"iphone\"}, {\"LOWER\": \"x\"}]\n",
        "\n",
        "# Token whose lowercase form matches 'iphone' and an optional digit\n",
        "pattern2 = [{\"LOWER\": \"iphone\"}, {\"IS_DIGIT\": True, \"OP\": \"?\"}]\n",
        "\n",
        "# Add patterns to the matcher\n",
        "matcher.add(\"GADGET\", None, pattern1, pattern2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-wX4t3SZ8WR6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "4f14338e-6bbe-4a87-aa94-5d05a6bd9830"
      },
      "source": [
        "TRAINING_DATA = []\n",
        "\n",
        "# Create a Doc object for each text in TEXTS\n",
        "for doc in nlp.pipe(TEXTS):\n",
        "    # Match on the doc and create a list of matched spans\n",
        "    spans = [doc[start:end] for match_id, start, end in matcher(doc)]\n",
        "    # Get (start character, end character, label) tuples of matches\n",
        "    entities = [(span.start_char, span.end_char, \"GADGET\") for span in spans]\n",
        "    # Format the matches as a (doc.text, entities) tuple\n",
        "    training_example = (doc.text, {\"entities\": entities})\n",
        "    # Append the example to the training data\n",
        "    TRAINING_DATA.append(training_example)\n",
        "\n",
        "print(*TRAINING_DATA, sep=\"\\n\")"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('How to preorder the iPhone X', {'entities': [(20, 28, 'GADGET'), (20, 26, 'GADGET')]})\n",
            "('iPhone X is coming', {'entities': [(0, 8, 'GADGET'), (0, 6, 'GADGET')]})\n",
            "('Should I pay $1,000 for the iPhone X?', {'entities': [(28, 36, 'GADGET'), (28, 34, 'GADGET')]})\n",
            "('The iPhone 8 reviews are here', {'entities': [(4, 10, 'GADGET'), (4, 12, 'GADGET')]})\n",
            "('Your iPhone goes up to 11 today', {'entities': [(5, 11, 'GADGET')]})\n",
            "('I need a new phone! Any tips?', {'entities': []})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "clAfPrXs9KFx",
        "colab_type": "text"
      },
      "source": [
        "### The training loop\n",
        "\n",
        " spaCy gives you full control over the training loop.\n",
        "\n",
        "**The steps of a training loop**\n",
        "1. `Loop` for a number of times.\n",
        "2. `Shuffle` the training data.\n",
        "3. `Divide` the data into batches.\n",
        "4. `Update` the model for each batch.\n",
        "5. `Save` the updated model.\n",
        "\n",
        "To prevent the model from getting stuck in a suboptimal solution, we randomly shuffle the data for each iteration. This is a very common strategy when doing stochastic gradient descent.\n",
        "\n",
        "Next, we divide the training data into batches of several examples, also known as minibatching. This makes it easier to make a more accurate estimate of the gradient.\n",
        "\n",
        "The label is what we want the model to predict. This can be a text category, or an entity span and its type.\n",
        "\n",
        "The gradient is how we should change the model to reduce the current error. It's computed when we compare the predicted label to the true label."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "brspytpQ9JOP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "import os\n",
        "path_to_model = '/content'\n",
        "TRAINING_DATA = [\n",
        "    (\"How to preorder the iPhone X\", {'entities': [(20, 28, 'GADGET')]})\n",
        "    # And many more examples...\n",
        "]\n",
        "\n",
        "# Loop for 10 iterations\n",
        "for i in range(10):\n",
        "    # Shuffle the training data\n",
        "    random.shuffle(TRAINING_DATA)\n",
        "    # Create batches and iterate over them\n",
        "    for batch in spacy.util.minibatch(TRAINING_DATA):\n",
        "        # Split the batch in texts and annotations\n",
        "        texts = [text for text, annotation in batch]\n",
        "        annotations = [annotation for text, annotation in batch]\n",
        "        # Update the model\n",
        "        nlp.update(texts, annotations)\n",
        "\n",
        "# Save the model\n",
        "nlp.to_disk(path_to_model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8wmpFmhF-hdK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "98a5d7a8-9786-420b-91bf-050bf01c5a42"
      },
      "source": [
        "!ls"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "meta.json  sample_data\ttokenizer  vocab\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qnkEwuSR-8uG",
        "colab_type": "text"
      },
      "source": [
        "**Updating an existing model**\n",
        "1. Improve the predictions on new data\n",
        "2. Especially useful to improve existing categories, like `PERSON`\n",
        "3. Also possible to add new categories\n",
        "4. Be careful and make sure the model doesn't \"forget\" the old ones"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJySUPua-wO2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        },
        "outputId": "c2230265-1367-417a-e2df-ed26099e7053"
      },
      "source": [
        "TRAINING_DATA = [\n",
        "    [\"How to preorder the iPhone X\", { \"entities\": [[20, 28, \"GADGET\"]] }],\n",
        "    [\"iPhone X is coming\", { \"entities\": [[0, 8, \"GADGET\"]] }],\n",
        "    [\"Should I pay $1,000 for the iPhone X?\", { \"entities\": [[28, 36, \"GADGET\"]] }],\n",
        "    [\"The iPhone 8 reviews are here\", { \"entities\": [[4, 12, \"GADGET\"]] }],\n",
        "    [\"Your iPhone goes up to 11 today\", { \"entities\": [[5, 11, \"GADGET\"]] }],\n",
        "    [\"I need a new phone! Any tips?\", { \"entities\": [] }]\n",
        "]\n",
        "\n",
        "\n",
        "# Start with blank English model\n",
        "nlp = spacy.blank('en')  # The blank model doesn't have any pipeline components, only the language data and tokenization rules.\n",
        "# Create blank entity recognizer and add it to the pipeline\n",
        "ner = nlp.create_pipe('ner')\n",
        "nlp.add_pipe(ner)\n",
        "# Add a new label\n",
        "ner.add_label('GADGET')\n",
        "\n",
        "# Start the training\n",
        "nlp.begin_training()\n",
        "\n",
        "# Loop for 10 iterations\n",
        "for itn in range(10):\n",
        "    # Shuffle the training data\n",
        "    random.shuffle(TRAINING_DATA)\n",
        "    losses = {}\n",
        "\n",
        "    # Batch the examples and iterate over them\n",
        "    for batch in spacy.util.minibatch(TRAINING_DATA, size=2):\n",
        "        texts = [text for text, entities in batch]\n",
        "        annotations = [entities for text, entities in batch]\n",
        "\n",
        "        # Update the model\n",
        "        nlp.update(texts, annotations, losses=losses)\n",
        "        print(losses)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'ner': 8.333333253860474}\n",
            "{'ner': 20.147342085838318}\n",
            "{'ner': 32.57593071460724}\n",
            "{'ner': 8.915273308753967}\n",
            "{'ner': 14.980735898017883}\n",
            "{'ner': 19.785090535879135}\n",
            "{'ner': 3.4871500469744205}\n",
            "{'ner': 8.14518128708005}\n",
            "{'ner': 9.895275425631553}\n",
            "{'ner': 3.050518828444183}\n",
            "{'ner': 4.285389910481172}\n",
            "{'ner': 6.3983944808423985}\n",
            "{'ner': 2.4975483752787113}\n",
            "{'ner': 4.363425818562973}\n",
            "{'ner': 6.49664493027376}\n",
            "{'ner': 2.532540822401643}\n",
            "{'ner': 3.292675310309278}\n",
            "{'ner': 6.6151768606796395}\n",
            "{'ner': 0.471284881750762}\n",
            "{'ner': 3.220182040666259}\n",
            "{'ner': 3.751443688146537}\n",
            "{'ner': 0.26872746776416534}\n",
            "{'ner': 1.3177758780420845}\n",
            "{'ner': 1.3331767641539045}\n",
            "{'ner': 0.002529067925934214}\n",
            "{'ner': 0.6114598607682638}\n",
            "{'ner': 0.6121194222692825}\n",
            "{'ner': 2.2857404061487614}\n",
            "{'ner': 2.285746621577806}\n",
            "{'ner': 2.286038732835405}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G0NtqzBDCKdg",
        "colab_type": "text"
      },
      "source": [
        "### Best practices for training spaCy models\n",
        "\n",
        "**Problem 1: Models can \"forget\" things**\n",
        "- Existing model can overfit on new data\n",
        "\n",
        "    e.g.: if we only update it with WEBSITE, it can \"unlearn\" what a PERSON is\n",
        "- Also known as \"catastrophic forgetting\" problem\n",
        "\n",
        "**Solution 1: Mix in previously correct predictions**\n",
        "\n",
        "For example, if we're training WEBSITE, also include examples of PERSON\n",
        "Run existing spaCy model over data and extract all other relevant entities\n",
        "\n",
        "**BAD:**\n",
        "\n",
        "    TRAINING_DATA = [\n",
        "      ('Reddit is a website', {'entities': [(0, 6, 'WEBSITE')]})\n",
        "      ]\n",
        "**GOOD:**\n",
        "\n",
        "    TRAINING_DATA = [\n",
        "      ('Reddit is a website', {'entities': [(0, 6, 'WEBSITE')]}),\n",
        "      ('Obama is a person', {'entities': [(0, 5, 'PERSON')]})\n",
        "    ]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vT_a7ogKC6tL",
        "colab_type": "text"
      },
      "source": [
        "**Problem 2: Models can't learn everything**\n",
        "- spaCy's models make predictions based on local context\n",
        "- Model can struggle to learn if decision is difficult to make based on context\n",
        "- Label scheme needs to be consistent and not too specific\n",
        "For example: `CLOTHING` is better than `ADULT_CLOTHING` and `CHILDRENS_CLOTHING`\n",
        "\n",
        "\n",
        "**Solution 2: Plan your label scheme carefully**\n",
        "- Pick categories that are reflected in local context\n",
        "- More generic is better than too specific\n",
        "- Use rules to go from generic labels to specific categories\n",
        "\n",
        "**BAD:**\n",
        "\n",
        "    LABELS = ['ADULT_SHOES', 'CHILDRENS_SHOES', 'BANDS_I_LIKE']\n",
        "**GOOD:**\n",
        "\n",
        "    LABELS = ['CLOTHING', 'BAND']\n"
      ]
    }
  ]
}